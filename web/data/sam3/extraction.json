{
  "document": "sam3",
  "source_file": "sam3.pdf",
  "extraction_date": "2025-12-16T22:17:51.958896",
  "model": "qwen3-vl-235b",
  "pages": [
    {
      "page_number": 1,
      "image": "page_01.png",
      "annotated_image": "page_01_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "Under review as a conference paper at ICLR 2026\nSAM 3: SEGMENT ANYTHING WITH CONCEPTS\nAnonymous authors\nPaper under double-blind review\nABSTRACT\nWe present Segment Anything Model (SAM) 3, a unified model that detects, seg-\nments, and tracks objects in images and videos based on concept prompts, which\nwe define as either short noun phrases (e.g., “yellow school bus”), image exemplars,\nor a combination of both. Promptable Concept Segmentation (PCS) takes such\nprompts and returns segmentation masks and unique identities for all matching\nobject instances. To advance PCS, we build a scalable data engine that produces\na high-quality dataset with 4M unique concept labels, including hard negatives,\nacross images and videos. Our model consists of a vision backbone shared be-\ntween an image-level detector and a memory-based video tracker. Recognition\nand localization are decoupled with a presence head, which significantly boosts\ndetection accuracy. SAM 3 delivers a 2× gain over existing systems in both image\nand video PCS, and improves previous SAM capabilities in interactive visual seg-\nmentation tasks. We open source SAM 3 along with our new Segment Anything\nwith Concepts (SA-Co) benchmark.\n1\nINTRODUCTION\nThe ability to find and segment anything in a visual scene is foundational for multimodal AI, powering\napplications in robotics, content creation, augmented reality, data annotation, and scientific fields.\nThe SAM series (Kirillov et al., 2023; Ravi et al., 2024) introduced the promptable segmentation task\nto segment objects in images and videos via interactive prompts, including visual inputs like points,\nboxes, and masks marking a specific object, or text inputs describing an object. However, SAM 1\nand SAM 2 focus on visual prompts and segment a single object instance per prompt. While these\nmethods achieved a breakthrough for this critical task, they did not address the broader task of finding\nand segmenting all instances of a concept appearing anywhere in the input (e.g., all “cats” in a video).\nIn this work, we present SAM 3, a model that achieves a step change in promptable segmentation in\nimages and videos, improving Promptable Visual Segmentation (PVS) relative to SAM 2 and setting\na new standard for Promptable Concept Segmentation (PCS). We formalize the PCS task as taking\ntext and/or image exemplars as input, and predicting instance and semantic masks for every single\nobject matching the concept, while preserving object identities across video frames (§2). We focus\non recognizing atomic visual concepts and thus constrain text to simple noun phrases (NPs), such as\n“red apple” or “striped cat”. Example outputs are shown in Fig. 1.\nFigure 1: SAM 3 improves over SAM 2 on promptable visual segmentation with clicks (left) while advancing\npromptable concept segmentation (right) where users can segment all instances of a visual concept specified by\na short noun phrase, image exemplars, or a combination of both.\n1\n",
      "text_length": 2962,
      "elements": [
        {
          "type": "figure",
          "bbox_1000": [
            199,
            721,
            805,
            865
          ],
          "bbox_pixels": [
            253,
            1189,
            1026,
            1427
          ],
          "label": "Figure 1",
          "description": "A comparative illustration showing SAM 3's improvements over SAM 2. Left side shows 'Promptable Visual Segmentation' with image and video examples using positive/negative points as prompts. Right side shows 'Promptable Concept Segmentation' with examples like 'a striped cat', 'a round cell', 'small window', 'a kangaroo', and 'hard hat', using noun phrases and/or image exemplars as prompts. The figure highlights the shift from segmenting single objects via clicks to segmenting all instances of a concept via text or image prompts.",
          "crop_path": "elements/p01_figure_1_Figure_1.png"
        }
      ]
    },
    {
      "page_number": 2,
      "image": "page_02.png",
      "annotated_image": "page_02_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "Under review as a conference paper at ICLR 2026\nFigure 2: Illustration of supported initial and optional interactive refinement prompts in the PCS task.\nWhile SAM 3 is not designed for long referring language expressions or queries requiring reasoning,\nwe show that it can be straightforwardly combined with a Multimodal Large Language Model\n(MLLM) to handle more complex language prompts.\nThe PCS task is inherently ambiguous due to its open-vocabulary nature, with many concepts having\nmultiple interpretations; e.g., “small window” is subjective (small vs large) and has ambiguous\nboundaries (with shutters vs without). Our approach systematically accounts for these ambiguities at\nevery stage, including data collection, metric design, and modeling. Consistent with previous SAM\nversions, SAM 3 is fully interactive, allowing users to resolve ambiguities by adding refinement\nprompts to guide the model towards their intended output.\nOur model (§3) consists of a detector and a tracker that share a vision encoder (Bolya et al., 2025).\nThe detector is a DETR-based (Carion et al., 2020) model conditioned on text, geometry, and image\nexemplars. To address the challenge of open-vocabulary concept detection, we introduce a separate\npresence head to decouple recognition and localization, which is especially effective when training\nwith challenging negative phrases. The tracker inherits the SAM 2 transformer encoder-decoder\narchitecture, supporting video segmentation and interactive refinement. The decoupled design for\ndetection and tracking avoids task conflict, as the detector needs to be identity agnostic, while the\ntracker’s main objective is to separate identities in the video.\nTo unlock major performance gains, we build a scalable human- and model-in-the-loop data en-\ngine (§4) that annotates a large and diverse training dataset. We innovate upon prior data engine\ndesigns in three key ways: i) media curation: we curate more diverse media domains than past\napproaches that rely on homogeneous web sources, ii) label curation: we significantly increase\nlabel diversity and difficulty by leveraging an ontology and multimodal LLMs as “AI annotators” to\ngenerate noun phrases and hard negatives, iii) label verification: we double annotation throughput by\nfine-tuning MLLMs to be effective “AI verifiers” that achieve near-human performance. Starting from\nnoisy media-phrase-mask pseudolabels, our data engine checks mask quality and exhaustivity using\nboth human and AI verifiers, filtering out correctly labeled examples and identifying challenging\nerror cases. Human annotators then focus on fixing these errors by manually correcting masks. This\nenables us to annotate high-quality training data with 4M unique phrases and 52M masks, and a\nsynthetic dataset with 38M phrases and 1.4B masks. We additionally create the Segment Anything\nwith Concepts (SA-Co) benchmark for PCS (§5) containing 214K unique concepts with exhaustive\nmasks in 124K images and 1.7K videos, > 50× more concepts than existing benchmarks.\nOur experiments (§6) show that SAM 3 sets a new state-of-the-art in promptable segmentation, e.g.,\nreaching a zero-shot mask AP of 47.0 on LVIS vs the current best of 38.5, surpassing baselines\non our new SA-Co benchmark by at least 2×, and improving upon SAM 2 on PVS benchmarks.\nAblations (§B) verify that the choice of backbone, novel presence head, and adding hard negatives\nall boost results, and establish scaling laws on the PCS task for both our high-quality and synthetic\ndatasets. We open-source the SA-Co benchmark and release the SAM 3 checkpoints and inference\ncode. On an H200 GPU, SAM 3 runs in 30 ms for a single image with 100+ detected objects. In\nvideo, the inference latency scales with the number of objects, sustaining near real-time performance\nfor ∼5 concurrent objects. Next we dive into the task, and review related work in §A.\n2\nPROMPTABLE CONCEPT SEGMENTATION (PCS)\nWe define the Promptable Concept Segmentation task as follows: given an image or short video (≤30\nsecs), detect, segment and track all instances of a visual concept specified by a short text phrase,\nimage exemplars, or a combination of both. We restrict concepts to those defined by simple noun\nphrases (NPs) consisting of a noun and optional modifiers. Noun-phrase prompts (when provided)\nare global to all frames of the image/video, while image exemplars can be provided on individual\nframes as positive or negative bounding boxes to iteratively refine the target masks (see Fig. 2).\n2\n",
      "text_length": 4516,
      "elements": [
        {
          "type": "figure",
          "bbox_1000": [
            225,
            98,
            778,
            189
          ],
          "bbox_pixels": [
            286,
            161,
            991,
            311
          ],
          "label": "Figure 2",
          "description": "Illustration of supported initial and optional interactive refinement prompts in the PCS task, showing four stages: initial prompt with text and box, output masks, refinement visual prompts with positive/negative examples, and refined masks.",
          "crop_path": "elements/p02_figure_1_Figure_2.png"
        }
      ]
    },
    {
      "page_number": 3,
      "image": "page_03.png",
      "annotated_image": "page_03_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "Under review as a conference paper at ICLR 2026\nFigure 3: SAM 3 architecture overview. See Fig. 8 for a more detailed diagram.\nAll prompts must be consistent in their category definition, or the model’s behavior is undefined; e.g.,\n“fish” cannot be refined with subsequent exemplar prompts of just the tail; instead the text prompt\nshould be updated. Exemplar prompts are particularly useful when the model initially misses some\ninstances, or when the concept is rare.\nOur vocabulary includes any simple noun phrase groundable in a visual scene, which makes the task\nintrinsically ambiguous. There can be multiple interpretations of phrases arising from polysemy\n(“mouse” device vs. animal), subjective descriptors (“cozy”, “large”), vague or context-dependent\nphrases that may not even be groundable (“brand identity”), boundary ambiguity (whether “mirror”\nincludes the frame), and factors like occlusion and blur that obscure object extent. While similar issues\nappear in large closed-vocabulary corpora (e.g., LVIS (Gupta et al., 2019)), they are alleviated by\ncarefully curating the vocabulary and setting a clear definition of all the classes of interest. We address\nthe ambiguity problem by collecting test annotations from three experts, adapting the evaluation\nprotocol to allow multiple valid interpretations (§F.3), designing the data pipeline and guidelines to\nminimize ambiguity during annotation, and introducing an ambiguity module in the model (§D.2).\n3\nMODEL\nSAM 3 is a generalization of SAM 2, supporting the new PCS task (§2) as well as the PVS task. It\ntakes concept prompts (simple noun phrases, image exemplars) or visual prompts (points, boxes,\nmasks) to define the objects to be (individually) segmented spatio-temporally. Image exemplars\nand visual prompts can be iteratively added on individual frames to refine the target masks—false\npositive and false negative objects can be removed or added respectively using image exemplars and\nan individual mask(let) can be refined using PVS in the style of SAM 2. Our architecture is broadly\nbased on the SAM and (M)DETR (Carion et al., 2020; Kamath et al., 2021) series. Fig. 3 shows the\nSAM 3 architecture, consisting of a dual encoder-decoder transformer—a detector for image-level\ncapabilities—which is used in combination with a tracker and memory for video. The detector and\ntracker ingest vision-language inputs from an aligned Perception Encoder (PE) backbone (Bolya\net al., 2025). We present an overview below, see §D for details.\nDetector Architecture. The architecture of the detector follows the general DETR paradigm. The\nimage and text prompt are first encoded by PE and image exemplars, if present, are encoded by\nan exemplar encoder. We refer to the image exemplar tokens and text tokens jointly as “prompt\ntokens”. The fusion encoder then accepts the unconditioned embeddings from the image encoder and\nconditions them by cross-attending to the prompt tokens. The fusion is followed by a DETR-like\ndecoder, where learned object queries cross-attend to the conditioned image embeddings from the\nfusion encoder. Each layer, for each object query, predicts a classification logit (in our case, a binary\nlabel of whether the object corresponds to the prompt), and a delta from the bounding box predicted\nby the previous level, following Zhu et al. (2020). We use box-region-positional bias (Lin et al., 2023)\nto help focalize the attention on each object, but unlike recent DETR models, we stick to vanilla\nattention. During training, we adopt dual supervision from DAC-DETR (Hu et al., 2023), and the\nAlign loss (Cai et al., 2024). The mask head is adapted from MaskFormer (Cheng et al., 2021). In\naddition, we also have a semantic segmentation head, which predicts a binary label for every pixel in\nthe image, indicating whether or not it corresponds to the prompt. See §D for details.\nPresence Token. It can be difficult for each of the proposal queries to both recognize (what) and\nlocalize (where) an object in the image/frame. For the recognition component, contextual cues from\nthe entire image are important. However, forcing proposal queries to understand the global context\ncan be counterproductive, as it conflicts with the inherently local nature of the localization objective.\n3\n",
      "text_length": 4272,
      "elements": [
        {
          "type": "figure",
          "bbox_1000": [
            112,
            99,
            802,
            238
          ],
          "bbox_pixels": [
            142,
            163,
            1022,
            392
          ],
          "label": "Figure 3",
          "description": "SAM 3 architecture overview diagram showing components like Text Encoder, Image Encoder, Detector, Tracker, Memory Bank, and their interactions for processing prompts and generating masks across frames. Includes visual elements such as exemplar images, masks, and color-coded data flow.",
          "crop_path": "elements/p03_figure_1_Figure_3.png"
        }
      ]
    },
    {
      "page_number": 4,
      "image": "page_04.png",
      "annotated_image": "page_04_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "Under review as a conference paper at ICLR 2026\nWe decouple the recognition and localization steps by introducing a learned global presence token.\nThis token is solely responsible for predicting whether the target concept in the form of a noun phrase\n(NP) is present in the image/frame, i.e. p(NP is present in input). Each proposal query qi only needs\nto solve the localization problem p(qi is a match | NP is present in input). The final score for each\nproposal query is the product of its own score and the presence score.\nImage Exemplars and Interactivity. SAM 3 supports image exemplars, given as a pair—a bounding\nbox and an associated binary label (positive or negative)—which can be used in isolation or to\nsupplement the text prompt. The model then detects all the instances that match the prompt. For\nexample, given a positive bounding box on a dog, the model will detect all dogs in the image. This\nis different from the PVS task in SAM 1 and 2, where a geometric/visual prompt yields only a\nsingle object instance. Each image exemplar is encoded separately by the exemplar encoder using\nan embedding for the position, an embedding for the label, and ROI-pooled visual features, then\nconcatenated and processed by a small transformer. The resulting prompt is concatenated to the text\nprompt to comprise the prompt tokens. Image exemplars can be interactively provided based on\nerrors in current detections to refine the output.\nTracker and Video Architecture. Given a video and a prompt P, we use the detector and a tracker\n(see Fig. 3) to detect and track objects corresponding to the prompt throughout the video. On each\nframe, the detector finds new objects Ot and the tracker propagates masklets Mt−1 (spatial-temporal\nmasks) from frames at the previous time t −1 to their new locations ˆ\nMt on the current frame at time\nt. We use a matching function to associate propagated masklets ˆ\nMt with new object masks emerging\nin the current frame Ot,\nˆ\nMt = propagate (Mt−1) ,\nOt = detect (It, P) ,\nMt = match and update\n\u0010\nˆ\nMt, Ot\n\u0011\n.\nTracking an Object with SAM 2 Style Propagation. A masklet is initialized for every object\ndetected on the first frame. Then, on each subsequent frame, the tracker module predicts the new\nmasklet locations\nˆ\nMt of those already-tracked objects based on their previous locations Mt−1\nthrough a single-frame propagation step similar to the video object segmentation task in SAM 2.\nThe tracker shares the same image/frame encoder (PE backbone) as the detector. After training the\ndetector, we freeze PE and train the tracker as in SAM 2, including a prompt encoder, mask decoder,\nmemory encoder, and a memory bank that encodes the object’s appearance using features from the\npast frames and conditioning frames (frames where the object is first detected or user-prompted). The\nmemory encoder is a transformer with self-attention across visual features on the current frame and\ncross-attention from the visual features to the spatial memory features in the memory bank.\nDuring inference, we only retain frames where the object is confidently present in the memory bank.\nThe mask decoder is a two-way transformer between the encoder hidden states and the output tokens.\nTo handle ambiguity, we predict three output masks for every tracked object on each frame along with\ntheir confidence, and select the most confident output as the predicted mask on the current frame.\nMatching and Updating Based on Detections. After obtaining the tracked masks ˆ\nMt, we match\nthem with the current frame detections Ot through a simple IoU based matching function (§D.3) and\nadd them to Mt on the current frame. We further spawn new masklets for all newly detected objects\nthat are not matched. The merging might suffer from ambiguities, especially in crowded scenes. We\naddress this with two temporal disambiguation strategies outlined next.\nFirst, we use temporal information in the form of a masklet detection score (§D.3) to measure how\nconsistently a masklet is matched to a detection within a temporal window (based on the number\nof past frames where it was matched to a detection). If a masklet’s detection score falls below a\nthreshold, we suppress it. Second, we use the detector outputs to resolve specific failure modes of the\ntracker due to occlusions or distractors. We periodically re-prompt the tracker with high-confidence\ndetection masks Ot, replacing the tracker’s own predictions ˆ\nMt. This ensures that the memory bank\nhas recent and reliable references (other than the tracker’s own predictions).\nInstance Refinement with Visual Prompts. After obtaining the initial set of masks (or masklets),\nSAM 3 allows refining individual masks(lets) using positive and negative clicks. Specifically, given\nthe user clicks, we apply the prompt encoder to encode them, and feed the encoded prompt into the\nmask decoder to predict an adjusted mask. In videos the mask is then propagated across the entire\nvideo to obtain a refined masklet.\n4\n",
      "text_length": 4974,
      "elements": [
        {
          "type": "equation",
          "bbox_1000": [
            162,
            400,
            836,
            430
          ],
          "bbox_pixels": [
            206,
            660,
            1065,
            709
          ],
          "label": "Equation 1",
          "description": "LaTeX: \\( \\hat{\\mathcal{M}}_t = \\text{propagate} \\left( \\mathcal{M}_{t-1} \\right), \\quad \\mathcal{O}_t = \\text{detect} \\left( I_t, P \\right), \\quad \\mathcal{M}_t = \\text{match\\_and\\_update} \\left( \\hat{\\mathcal{M}}_t, \\mathcal{O}_t \\right) \\)",
          "crop_path": "elements/p04_equation_1_Equation_1.png",
          "rendered_path": "elements/p04_equation_1_Equation_1_rendered.png"
        }
      ],
      "extraction_time_seconds": 66.14,
      "total_page_time_seconds": 66.27
    },
    {
      "page_number": 5,
      "image": "page_05.png",
      "annotated_image": "page_05_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "Under review as a conference paper at ICLR 2026\nFigure 4: Overview of the final SAM 3 data engine. See §F.1 for details of collected data.\nTraining Stages.\nWe train SAM 3 in four stages that progressively add data and capabilities:\n1) Perception Encoder (PE) pre-training, 2) detector pre-training, 3) detector fine-tuning, and 4) tracker\ntraining with a frozen backbone. See §D.4.1 for details.\n4\nDATA ENGINE\nAchieving a step change on PCS with SAM 3 requires training on a large and diverse set of concepts\nand visual domains, beyond any existing dataset (see Figure 10). Our solution is to build an efficient\ndata engine that iteratively generates annotated data via a feedback loop with SAM 3, human\nannotators, and AI annotators, actively mining media-phrase pairs on which the current version of\nSAM 3 fails to produce high-quality training data to further improve the model. By delegating certain\ntasks to AI annotators—models that match or surpass human accuracy—we more than double the\nthroughput compared to a human-only annotation pipeline. We develop the data engine in four phases,\nwith each phase increasing the use of AI models to steer human effort to the most challenging failure\ncases, alongside expanding visual domain coverage. Phases 1-3 focus only on images, with Phase 4\nexpanding to videos. We describe the key steps here; details and metrics are in §E.\nData Engine Components (Fig. 4). Media inputs (image or video) are mined from a large pool\nwith the help of a curated ontology. An AI model proposes noun phrases (NPs) describing visual\nconcepts, followed by another model (e.g., SAM 3) that generates candidate instance masks for each\nproposed NP. The proposed masks are verified by a two-step process: first, in Mask Verification\n(MV) annotators accept or reject masks based on their quality and relevance to the NP. Second, in\nExhaustivity Verification (EV) annotators check if all instances of the NP have been masked in the\ninput. Any media-NP pairs that did not pass the exhaustivity check are sent to a manual correction\nstage, where humans add, remove or edit masks (using SAM 1 in a browser based tool), or use “group”\nmasks for small, hard to separate objects. Annotators may reject ungroundable or ambiguous phrases.\nPhase 1: Human Verification. At first, data mining is done by randomly sampling images and NP\nproposal is done with a simple captioner and parser. The initial mask proposal model is SAM 2\nprompted with the output of an off-the-shelf open-vocabulary detector, and verifiers are all human.\nThe collected 4.3M image-NP pairs form the initial SA-Co/HQ dataset. We train SAM 3 on this data\nand use it as the mask proposal model for the next phase.\nPhase 2: Human + AI Verification. In this next phase, we use human accept/reject labels from the\nMV and EV tasks collected in Phase 1 to fine-tune Llama 3.2 (Dubey et al., 2024) to create AI verifiers\nto automatically perform the MV and EV tasks. These models receive image-phrase-mask triplets\nand produce multiple-choice judgements of mask quality or exhaustivity. This new auto-verification\nprocess allows human effort to be focused on the most challenging cases. We continue to re-train\nSAM 3 on newly collected data and update it 6 times. As SAM 3 and AI verifiers improve, a higher\nproportion of labels are auto-generated, further accelerating data collection. The introduction of AI\nverifiers for MV and EV doubles the data engine’s throughput. We further upgrade the NP proposal\nstep to a Llama-based pipeline that also proposes hard negative NPs adversarial to SAM 3. This\nphase adds 122M image-NP pairs to SA-Co/HQ.\nPhase 3: Scaling and Domain Expansion. In this phase, we use AI models to mine increasingly\nchallenging cases and broaden domain coverage in SA-Co/HQ to 15 datasets (Fig. 11). In new\ndomains, the MV AI verifier performs well zero-shot, while the EV AI verifier improves with modest\ndomain-specific human supervision. We also expand concept coverage to long-tail, fine-grained\nconcepts by extracting NPs from each image’s alt-text where available and by mining concepts from\na 22.4M node SA-Co ontology based on Wikidata (17 top-level categories, 72 sub-categories). We\niterate SAM 3 training 7 times and AI verifiers 3 times, and add 19.5M image-NP pairs to SA-Co/HQ.\n5\n",
      "text_length": 4294,
      "elements": [
        {
          "type": "figure",
          "bbox_1000": [
            225,
            100,
            925,
            214
          ],
          "bbox_pixels": [
            286,
            165,
            1179,
            353
          ],
          "label": "Figure 4",
          "description": "Overview of the final SAM 3 data engine, showing a flowchart with components such as Media Pool, Mine data, Propose NPs, Propose mask (SAM 3), AI verifiers, Human verifiers, and final annotations. Includes icons for data, AI model, human, label, and train, with arrows indicating the data flow and feedback loop.",
          "crop_path": "elements/p05_figure_1_Figure_4.png"
        }
      ]
    },
    {
      "page_number": 6,
      "image": "page_06.png",
      "annotated_image": "page_06_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "Under review as a conference paper at ICLR 2026\nFigure 5: Example video (top) and images (bottom) from SA-Co with annotated phrases and instance masks/IDs.\nPhase 4: Video Annotation. This phase extends the data engine to video. We use a mature SAM 3 to\ncollect targeted quality annotations that capture video-specific challenges. The data mining pipeline\napplies scene/motion filters, content balancing, ranking, and targeted searches. Video frames are\nsampled (randomly or by object density) and sent to the image annotation flow (from phase 3).\nMasklets are produced with SAM 3 (now extended to video) and post-processed via deduplication\nand removal of trivial masks. Because video annotation is more difficult, we concentrate humans on\nlikely failures by favoring clips with many crowded objects and tracking failures. The collected video\ndata SA-Co/VIDEO consists of 52.5K videos and 467K masklets. See §E.6 for details.\n5\nSEGMENT ANYTHING WITH CONCEPTS (SA-CO) DATASET\nTraining Data. We collect three image datasets for the PCS task: i) SA-Co/HQ, the high-quality\nimage data collected from the data engine in phases 1-4, ii) SA-Co/SYN, a synthetic dataset of images\nlabeled by a mature version of the data engine (phase 3) without any human involvement, and iii)\nSA-Co/EXT, fifteen external datasets which have instance mask annotations, enriched with hard\nnegatives using our SA-Co ontology pipeline. Notably in the SA-Co/HQ dataset we annotate 5.2M\nimages and 4M unique NPs, making it the largest high-quality open-vocab segmentation dataset.\nWe also annotate a video dataset, SA-Co/VIDEO, containing 52.5K videos and 24.8K unique NPs,\nforming 134K video-NP pairs. The videos have an average length of 84.1 frames at 6 fps. See §F.1\nfor details including full statistics, comparison with existing datasets and the distribution of concepts.\nSA-Co Benchmark. The SA-Co evaluation benchmark has all together 214K unique phrases, 126K\nimages and videos, and over 3M media-phrase pairs with challenging hard negative labels to test\nopen-vocabulary recognition. It consists of several splits: SA-Co/Gold has seven domains and\neach image-NP pair is annotated by three different annotators (used measure human performance);\nSA-Co/Silver has ten domains and only one human annotation per image-NP pair; SA-Co/Bronze\nand SA-Co/Bio are nine existing datasets either with existing mask annotations or masks generated\nby using boxes as prompts to SAM 2. The SA-Co/VEval benchmark has three domains and one\nannotator per video-NP pair. See Tab. 29 for dataset statistics and Fig. 5 for example annotations.\nMetrics. We aim to measure the usefulness of the model in downstream applications. Detection\nmetrics such as average precision (AP) do not account for calibration, which means that models\ncan be difficult to use in practice. To remedy this, we only evaluate predictions with confidence\nabove 0.5, effectively introducing a threshold that mimics downstream usages and enforces good\ncalibration. The PCS task can be naturally split into two sub-tasks, localization and classification.\nWe evaluate localization using positive macro F1 (pmF1) on positive media-phrase pairs with at least\none ground-truth and one predicted mask. Classification is measured with image-level Matthews\nCorrelation Coefficient (IL MCC) which ranges in [−1, 1] and evaluates binary prediction at the\nimage level (“is the object present or not?”) without regard for the quality of the masks. Our main\nmetric, classification-gated F1 (CGF1), combines these as follows: CGF1 = 100∗pmF1 ∗IL MCC.\nFull definitions are in §F.3.\n6\n",
      "text_length": 3599,
      "elements": [
        {
          "type": "figure",
          "bbox_1000": [
            210,
            93,
            788,
            331
          ],
          "bbox_pixels": [
            267,
            153,
            1004,
            546
          ],
          "label": "Figure 5",
          "description": "Example video (top) and images (bottom) from SA-Co with annotated phrases and instance masks/IDs. The top row shows four video frames with color-coded object masks and corresponding labels (e.g., 'a tree', 'the fabric', 'a yellow shirt'). The bottom row displays three images with similar annotations, including items like 'plastic bag', 'pomegranate', 'a chain', 'a MacBook', and 'white iPhone', each with color-coded masks and labels.",
          "crop_path": "elements/p06_figure_1_Figure_5.png"
        }
      ]
    },
    {
      "page_number": 7,
      "image": "page_07.png",
      "annotated_image": "page_07_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "Under review as a conference paper at ICLR 2026\nInstance Segmentation\nBox Detection\nSemantic Segmentation\nLVIS\nSA-Co\nLVIS\nCOCO\nSA-Co\nADE-847 PC-59 Cityscapes\nModel\nCGF1 AP\nGold Silver Bronze Bio\nCGF1\nAP\nAP\nAPo\nGold Silver Bronze Bio\nmIoU\nmIoU\nmIoU\nHumanmin\n–\n–\n74.2\n–\n–\n–\n–\n–\n–\n–\n76.2\n–\n–\n–\n–\n–\n–\nHumanmax\n–\n–\n81.4\n–\n–\n–\n–\n–\n–\n–\n83.6\n–\n–\n–\n–\n–\n–\nOWLv2\n35.5\n–\n22.9\n12.1\n8.5\n1.1\n35.2\n35.2\n38.2\n42.4\n23.3\n11.7\n8.5\n1.6\n–\n–\n–\nOWLv2⋆\n45.7\n43.4\n34.3\n19.3\n19.3\n0.2\n47.4\n45.5\n46.1\n23.9\n35.4\n19.2\n19.4\n0.3\n–\n–\n–\ngDino-T\n32.9\n–\n9.1\n7.4\n11.1\n0.6\n33.8\n20.5\n45.7\n35.3\n9.4\n6.8\n12.3\n0.7\n–\n–\n–\nLLMDet-L 48.1\n36.3\n12.9\n12.1\n18.8\n0.5\n53.7\n42.0\n55.6\n49.8\n13.5\n11.5\n20.6\n0.6\n–\n–\n–\nAPE-D⋆\n–\n53.0† 27.3\n15.0\n19.7\n0.0\n–\n59.6† 58.3†\n–\n29.4\n15.9\n21.8\n0.0\n9.2†\n58.5†\n44.2†\nDINO-X\n–\n38.5† 27.7δ\n–\n–\n–\n–\n52.4† 56.0†\n–\n29.4δ\n–\n–\n–\n–\n–\n–\nGemini 2.5 19.8\n–\n16.4\n10.5\n8.9\n10.2\n23.7\n–\n–\n–\n18.7\n12.2\n10.1\n12.0\n–\n–\n–\nSAM 3\n52.8\n47.0\n65.0\n57.1\n49.5\n59.3\n57.5\n51.7\n53.5\n55.5\n67.7\n58.0\n53.1\n60.0\n14.7\n59.4\n65.1\nTable 1: Evaluation on image concept segmentation with text. APo corresponds to COCO-O accuracy, ⋆partially\ntrained on LVIS. †from original papers, δfrom DINO-X API. Gray numbers indicate usage of respective closed\nset training data (LVIS/COCO). Upper and lower bound for human performance given, see §F.4 for details.\nHandling Ambiguity. We collect 3 annotations per NP on SA-Co/Gold. We measure oracle accuracy\ncomparing each prediction to all ground truths and selecting the best score. See §F.3.\n6\nEXPERIMENTS\nWe evaluate SAM 3 across image/video segmentation, few-shot adaptation to detection and counting\nbenchmarks, and segmenting from complex language queries with SAM 3 + MLLM. We also show a\nsubset of ablations, with more in §B. References, more results and details are in §G.\nImage PCS with Text. We evaluate instance segmentation, box detection, and semantic segmentation\non external and our benchmarks. SAM 3 is prompted with a single NP at a time, and predicts instance\nmasks, bounding boxes, or semantic masks. As baselines, we evaluate OWLv2, GroundingDino, and\nLLMDet on box detection, and prompt SAM 1 with their boxes to evaluate segmentation. We also\ncompare to APE, DINO-X, and Gemini 2.5 Flash, a generalist LLM. Tab. 1 shows that zero-shot,\nSAM 3 is competitive on closed-vocabulary COCO, COCO-O and on LVIS boxes, and is significantly\nbetter on LVIS masks. On open-vocabulary SA-Co/Gold SAM 3 achieves double the CGF1 score of\nthe strongest baseline OWLv2⋆, and 88% of the estimated lower bound on human performance. The\nimprovements are even higher on the other SA-Co splits. Open vocabulary semantic segmentation\nresults on ADE-847, PascalConcept-59, and Cityscapes show that SAM 3 outperforms APE, a strong\nspecialist baseline. See §G.1 for details.\nODinW13\nRF-100VL\nModel\nAP0\nAP10 AP0\nAP10\nGemini2.5-Pro\n33.7\n–\n11.6\n9.8\ngDino-T\n49.7\n–\n15.7\n33.7\ngDino1.5-Pro\n58.7\n67.9\n–\n–\nSAM 3\n59.9\n71.6\n14.3\n35.7\nTable 2: Zero-shot and 10-shot\ntransfer on in-the-wild datasets.\nCOCO\nLVIS\nODinW13\nAP\nAP+\nAP+\nAP+\nAP\nAP+\nAP+\nAP+\nAP\nAP+\nAP+\nAP+\nModel\nT\nT\nI\nT+I\nT\nT\nI\nT+I\nT\nT\nI\nT+I\nT-Rex2\n52.2\n–\n58.5\n–\n45.8\n–\n65.8\n–\n50.3\n–\n61.8\n–\nSAM 3\n53.5\n56.8\n75.7\n76.0\n51.7\n53.4\n75.5\n77.0\n59.9\n62.5\n81.9\n79.6\nTable 3: Prompting with 1 exemplar on COCO, LVIS and ODinW35. Eval-\nuation per prompt type: T (text-only), I (image-only), and T+I (combined\ntext and image). AP+ is evaluated only on positives examples.\nFew-Shot Adaptation. We evaluate zero- and few-shot transfer of SAM 3 on ODinW13 and\nRoboflow100-VL, with their original labels as prompts. We do not perform any prompt tuning.\nWe fine-tune SAM 3 without mask loss, and report average bbox mAP in Tab. 2. SAM 3 achieves\nSoTA 10-shot performance, surpassing in-context prompting in Gemini and object detection experts\n(gDino); more details in §G.3. RF-100VL contains domains with specialized prompts that are out of\nSAM 3’s current scope, but SAM 3 adapts through fine-tuning more efficiently than baselines.\nPCS with 1 Exemplar. We first evaluate image exemplars using a single input box sampled at\nrandom from the ground truth. This can be done only on “positive” data, where each prompted object\nappears in the image. We report the corresponding AP+ in Tab. 3 across three settings: text prompt\n(T), exemplar image (I), and both text and image (T+I); SAM 3 outperforms prior state-of-the-art\nT-Rex2 by a healthy margin on COCO (+17.2), LVIS (+9.7), and ODinW (+20.1). See §G.2 for more\ndetails and results on SA-Co/Gold.\n7\n",
      "text_length": 4455,
      "elements": [
        {
          "type": "table",
          "bbox_1000": [
            169,
            100,
            835,
            248
          ],
          "bbox_pixels": [
            215,
            165,
            1064,
            409
          ],
          "label": "Table 1",
          "description": "Table 1: Evaluation on image segmentation with text. Contains performance metrics (CGF1, AP, Gold, Silver, Bronze, Bio, etc.) for various models (Human_min, Human_max, OWLv2, OWLv2*, gDino-T, LLMdet-L, APE-D*, DINO-X, Gemini 2.5, SAM 3) across tasks like Instance Segmentation (LVIS, SA-Co), Box Detection (LVIS, COCO, SA-Co), and Semantic Segmentation (ADE-847, PC-59, Cityscapes). Includes notes on oracle accuracy, training data usage, and human performance bounds.",
          "crop_path": "elements/p07_table_1_Table_1.png"
        },
        {
          "type": "table",
          "bbox_1000": [
            169,
            620,
            375,
            688
          ],
          "bbox_pixels": [
            215,
            1023,
            478,
            1135
          ],
          "label": "Table 2",
          "description": "Table 2: Zero-shot and 10-shot transfer on in-the-wild datasets. Shows AP0 and AP10 scores for models (Gemini2.5-Pro, gDino-T, gDino1.5-Pro, SAM 3) on ODinW13 and RF-100VL benchmarks.",
          "crop_path": "elements/p07_table_2_Table_2.png"
        },
        {
          "type": "table",
          "bbox_1000": [
            385,
            620,
            835,
            688
          ],
          "bbox_pixels": [
            490,
            1023,
            1064,
            1135
          ],
          "label": "Table 3",
          "description": "Table 3: Prompting with 1 exemplar on COCO, LVIS, and ODinW35. Evaluates AP and AP+ scores for models (T-Rex2, SAM 3) under different prompt types: T (text-only), I (image-only), T+I (combined text and image). AP+ is evaluated only on positive examples.",
          "crop_path": "elements/p07_table_3_Table_3.png"
        }
      ]
    },
    {
      "page_number": 8,
      "image": "page_08.png",
      "annotated_image": "page_08_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "Under review as a conference paper at ICLR 2026\n0\n1\n2\n3\n4\n5\n6\n7\nNumber of Prompts\n65\n70\n75\n80\n85\nCGF1 (SA-Co/Gold)\nSAM 3 PCS\nPerfect SAM 3 PVS\nHybrid: 2 prompts PCS then PVS\nHybrid: 4 prompts PCS then PVS\nFigure 6: SAM 3’s interactive exemplar\nprompts vs the ideal PVS baseline on SA-\nCo. We report CGF1 vs # of box prompts,\naveraged over all SA-Co/Gold phrases.\nPCS with K Exemplars. Next, we evaluate SAM 3 in\nan interactive setting, simulating collaboration with a hu-\nman annotator. Starting with a text prompt, we iteratively\nadd one exemplar prompt at a time: missed ground truths\nare candidate positive prompts, false positive detections\nare candidate negative prompts. Results (Fig. 6) are com-\npared to a perfect PVS baseline, where we simulate the\nuser manually fixing errors using ideal box-to-mask cor-\nrections. SAM 3’s PCS improves CGF1 more quickly, as\nit generalizes from exemplars (e.g., detecting or suppress-\ning similar objects), while PVS only corrects individual\ninstances. After 3 clicks, interactive PCS outperforms\ntext-only by +18.6 CGF1 points and PVS refinement by\n+9.7. Performance plateaus after 4 clicks, as exemplars\ncannot fix poor-quality masks. Simulating a hybrid switch\nto PVS at this point yields further gains, showing that the\ntwo approaches are complementary.\nCountBench\nPixMo-Count\nModel\nMAE ↓\nAcc ↑\nMAE ↓\nAcc ↑\nDINO-X\n0.62\n82.9\n0.21\n85.0\nQwen2-VL-72B\n0.28\n86.7\n0.61\n63.7\nMolmo-72B\n0.27\n92.4\n0.17\n88.8\nGemini 2.5 Pro\n0.24\n92.4\n0.38\n78.2\nSAM 3\n0.11\n95.6\n0.22\n87.3\nTable 4: Accuracy on counting benchmarks.\nGray indicates usage of training sets.\nObject Counting. We evaluate on object counting bench-\nmarks CountBench and PixMo-Count to compare with\nseveral MLLMs using Accuracy (%) and Mean Absolute\nError (MAE) from previous technical reports and our own\nevaluations. See Tab. 4 for results and §G.4 for more eval-\nuation details. Compared to MLLMs, SAM 3 not only\nachieves good object counting accuracy, but also provides\nobject segmentation that most MLLMs cannot provide.\nVideo PCS with Text. We evaluate video segmentation with text prompts on both our SA-Co/VEval\nbenchmark and existing public benchmarks. For SA-Co/VEval, we report CGF1 and pHOTA\nmetrics (defined in §G.5) across its subsets (SA-V, YT-Temporal-1B, SmartGlasses). For public\nbenchmarks, we use their official metrics. Baselines include GLEE, an open-vocabulary image and\nvideo segmentation model, “LLMDet + SAM 3 Tracker” (replacing our detector with LLMDet),\nand “SAM 3 Detector + T-by-D” (replacing our tracker with an association module based on the\ntracking-by-detection paradigm). In Tab. 5, SAM 3 largely outperforms these baselines, especially\non benchmarks with a very large number of noun phrases. On SA-Co/VEval it reaches over 80% of\nhuman pHOTA. See §G.5 for more details.\nSA-Co/VEval benchmark test split\npublic benchmarks\nSA-V\nYT-Temporal-1B\nSmartGlasses\nLVVIS\nBURST\nYTVIS21\nOVIS\n(2.0K NPs)\n(1.7K NPs)\n(2.4K NPs)\n(1.2K NPs)\n(482 NPs)\n(40 NPs)\n(25 NPs)\nModel\nCGF1\npHOTA\nCGF1\npHOTA\nCGF1\npHOTA\ntest mAP\ntest HOTA\nval mAP\nval mAP\nHuman\n53.2\n68.0\n73.8\n79.2\n57.6\n70.3\n–\n–\n–\n–\nGLEE† (all NPs at once)\n0.2\n8.8\n2.1\n18.9\n0.1\n5.5\n20.8\n28.4\n62.2\n38.7\nGLEE† (one NP at a time)\n0.0\n12.3\n3.2\n22.6\n0.1\n6.4\n9.3\n20.2\n56.5\n32.4\nLLMDet† + SAM 3 Tracker\n3.6\n31.2\n9.2\n41.5\n0.0\n7.4\n15.5\n35.1\n32.4\n47.5\nSAM 3 Detector + T-by-D\n22.2\n49.0\n44.6\n64.4\n29.3\n57.1\n37.3\n40.7\n57.3\n54.9\nSAM 3\n27.8\n53.9\n51.7\n69.2\n38.2\n62.9\n38.2\n45.9\n56.9\n59.9\nTable 5: Video PCS from a text prompt (open-vocabulary video instance segmentation) on SA-Co/VEval and\npublic benchmarks (see Tab. 38 for more results and analyses). SAM 3 shows strong performance, especially on\nbenchmarks with a large number of NPs. †: GLEE and LLMDet have not been trained on the SA-Co dataset, so\ntheir results should be seen as zero-shot on SA-Co/VEval.\nPVS. We evaluate SAM 3 on a range of geometric tasks, including Video Object Segmentation (VOS)\nand interactive image segmentation. Tab. 6 compares SAM 3 to recent state-of-the-art methods on the\nVOS task. SAM 3 achieves significant improvements over SAM 2 on most benchmarks, particularly\non the challenging MOSEv2 dataset, where SAM 3 outperforms prior work by 6 points. For the\ninteractive image segmentation task, we evaluate SAM 3 on the 37 datasets benchmark introduced in\nSAM 2. As shown in Tab. 7, SAM 3 outperforms SAM 2 on average mIoU.\nSAM 3 Agent.\nWe experiment with an MLLM that uses SAM 3 as a tool, called SAM 3 Agent,\nto segment more complex text queries such as “people sitting down but not holding a gift box in\n8\n",
      "text_length": 4544,
      "elements": [
        {
          "type": "figure",
          "bbox_1000": [
            571,
            98,
            824,
            251
          ],
          "bbox_pixels": [
            728,
            161,
            1050,
            414
          ],
          "label": "Figure 6",
          "description": "Line chart comparing SAM 3's interactive exemplar prompts vs. ideal PVS baseline on SA-Co, showing CGF1 score vs. number of box prompts, averaged over all SA-Co/Gold phrases. Four lines represent: SAM 3 PCS (blue), Perfect SAM 3 PVS (orange), Hybrid: 2 prompts PCS then PVS (teal), and Hybrid: 4 prompts PCS then PVS (magenta).",
          "crop_path": "elements/p08_figure_1_Figure_6.png"
        },
        {
          "type": "table",
          "bbox_1000": [
            565,
            344,
            833,
            438
          ],
          "bbox_pixels": [
            720,
            567,
            1062,
            722
          ],
          "label": "Table 4",
          "description": "Table showing accuracy on counting benchmarks (CountBench and PixMo-Count) for models including DINO-X, Qwen2-VL-72B, Molo-72B, Gemini 2.5 Pro, and SAM 3, with metrics MAE ↓ and Acc ↑. Gray shading indicates usage of training sets.",
          "crop_path": "elements/p08_table_2_Table_4.png"
        },
        {
          "type": "table",
          "bbox_1000": [
            175,
            603,
            824,
            735
          ],
          "bbox_pixels": [
            223,
            994,
            1050,
            1212
          ],
          "label": "Table 5",
          "description": "Table comparing video PCS performance (open-vocabulary video instance segmentation) on SA-Co/V Eval and public benchmarks for models including Human, GLEE, LLMdet + SAM 3 Tracker, SAM 3 Detector + T-by-D, and SAM 3. Metrics include CGF1, pHOTA, test mAP, val mAP, etc., across datasets like SA-V, YT-Temporal-1B, SmartGlasses, LVVIS, BURST, YTVIS21, and OVIS.",
          "crop_path": "elements/p08_table_3_Table_5.png"
        },
        {
          "type": "chart",
          "bbox_1000": [
            571,
            98,
            824,
            251
          ],
          "bbox_pixels": [
            728,
            161,
            1050,
            414
          ],
          "label": "Figure 6",
          "description": "Line chart comparing SAM 3's interactive exemplar prompts vs. ideal PVS baseline on SA-Co, showing CGF1 score vs. number of box prompts, averaged over all SA-Co/Gold phrases. Four lines represent: SAM 3 PCS (blue), Perfect SAM 3 PVS (orange), Hybrid: 2 prompts PCS then PVS (teal), and Hybrid: 4 prompts PCS then PVS (magenta).",
          "crop_path": "elements/p08_chart_4_Figure_6.png"
        }
      ]
    },
    {
      "page_number": 9,
      "image": "page_09.png",
      "annotated_image": "page_09_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "Under review as a conference paper at ICLR 2026\nJ &F\nG\nJ & ˙F\nModel\nMOSEv1\nval\nDAVIS17\nval\nLVOSv2\nval\nSA-V\nval\nSA-V\ntest\nYTVOS19\nval\nMOSEv2\nval\nSAMURAI\n72.6\n89.9\n84.2\n79.8\n80.0\n88.3\n51.1\nSAM2Long\n75.2\n91.4\n85.9\n81.1\n81.2\n88.7\n51.5\nSeC\n75.3\n91.3\n86.5\n82.7\n81.7\n88.6\n53.8\nSAM 2.1 L\n77.9\n90.7\n79.6\n77.9\n78.4\n89.3\n47.9†\nSAM 3\n78.4\n92.0\n88.2\n82.9\n84.6\n89.6\n60.1\nTable 6: SAM 3 improves over SAM 2 in VOS. †: Zero-shot.\nAvg. mIoU\nModel\n1-click\n5-clicks\nSAM 1 H\n58.5\n82.1\nSAM 2.1 L\n64.8\n84.4\nSAM 3\n65.4\n85.0\nTable 7: Interactive image segmen-\ntation on the SA-37 benchmark.\nReasonSeg (gIoU)\nOmnilabel (AP)\nval\ntest\nval 2023\nModel\nMLLM\nAll\nAll\nShort\nLong\ndescr\ndescr-S\ndescr-M\ndescr-L\nX-SAM\nPhi-3-3.8B\n56.6\n57.8\n47.7\n56.0\n12.0*\n17.1*\n11.4*\n8.8*\nSegZero\nQwen2.5-VL 7B\n62.6\n57.5\n–\n–\n13.5*\n20.7*\n12.4*\n9.1*\nRSVP\nGPT-4o\n64.7\n55.4\n61.9\n60.3\n–\n–\n–\n–\nOverall SoTA Performance†\n65\n61.3\n55.4\n63.2\n36.5\n54.4\n33.2\n25.5\nSAM 3 Agent\nQwen2.5-VL 7B\n65.4\n62.6\n59.1\n63.7\n36.5\n52.6\n34.3\n26.7\nSAM 3 Agent\nLlama4 Maverick\n71.5\n69.3\n70.9\n68.8\n36.2\n47.5\n34.9\n28.1\nSAM 3 Agent\nQwen2.5-VL 72B\n75.0\n71.8\n71.3\n72.0\n44.7\n58.4\n42.6\n36.1\nSAM 3 Agent\nGemini 2.5 Pro\n76.0\n73.8\n74.0\n73.7\n46.7\n54.6\n46.2\n38.7\nTable 8: SAM 3 Agent results. Gray indicates fine-tuned results on ReasonSeg (train), * indicates reproduced\nresults, underline indicates the main metric. †: LISA-13B-LLaVA1.5 for ReasonSeg; REAL for OmniLabel.\ntheir hands”. The MLLM proposes noun phrase queries to prompt SAM 3 and analyzes the returned\nmasks, iterating until the masks are satisfactory. Tab. 8 shows that SAM 3 Agent evaluated zero-\nshot on ReasonSeg (Lai et al., 2024) and OmniLabel (Schulter et al., 2023) surpasses prior work\nwithout training on any referring expression segmentation or reasoning segmentation data. SAM 3\nAgent also outperforms previous zero-shot results on RefCOCO+ (Kazemzadeh et al., 2014) and\nRefCOCOg (Mao et al., 2016). SAM 3 can be combined with various MLLMs, with the same set of\nthe system prompts for all those MLLMs, showing SAM 3’s robustness. See §H for more details.\nCGF1 IL MCC pmF1\n×\n57.6\n0.77\n74.7\n✓\n63.3\n0.82\n77.1\n(a) Presence token.\n#/img CGF1 IL MCC pmF1\n0\n31.8\n0.44\n70.2\n5\n44.8\n0.62\n71.9\n30\n49.2\n0.68\n72.3\n(b) Hard Negatives.\nEXT SYN HQ CGF1 IL MCC pmF1\n✓\n×\n×\n30.9\n0.46\n66.3\n✓\n✓\n×\n39.7\n0.57\n70.6\n✓\n×\n✓\n51.8\n0.71\n73.2\n✓\n✓\n✓\n54.3\n0.74\n73.5\n(c) Training data.\nModel\nCGF1 IL MCC pmF1\nHumanmin\n74.2\n0.88\n84.7\nSAM 3\n65.0\n0.82\n79.7\n+ EV AI\n68.1\n0.86\n78.8\n+ MV AI\n69.2\n0.85\n81.3\n(d) SAM 3 + AI verifiers.\nTable 9: Selected model and data ablations on SA-Co/Gold. Numbers across tables are not directly comparable.\nSelected Ablations. In Tab. 9 we report a subset of the more extensive ablations from §B. The\npresence head boosts CGF1 by +5.7 (9a), mainly improving image-level recognition ability measured\nby IL MCC. Tab. 9b shows that adding hard negatives significantly improves the model performance,\nmost notably the image-level IL MCC from 0.44 to 0.68. Tab. 9c shows that synthetic (SYN) training\ndata improves over the external (EXT) by +8.8 CGF1 and our high-quality (HQ) annotations add\n+14.6 CGF1 on top of this baseline. We present detailed data scaling laws of the synthetic data and\nhigh-quality data in §B.2, showing their effectiveness on both in-domain and out-of-domain test sets.\nIn Tab. 9d, we show how AI verifiers can improve pseudo-labels. Replacing the presence score from\nSAM 3 with a presence score from the exhaustivity verification (EV) AI verifier boosts IL MCC by\n4.6 points. Using the mask verification (MV) AI verifier to remove bad masks boosts pmF1 by +2.5.\n7\nCONCLUSION\nWe present Segment Anything with Concepts, enabling open vocabulary text and exemplars as\nprompts in interactive segmentation. Our principal contributions are: (i) introducing the PCS task and\nSA-Co benchmark, (ii) a decoupled recognition-localization architecture that extends SAM 2 for PCS\nwhile retaining PVS capabilities, (iii) a high-quality efficient human and AI annotator in the loop data\nengine. SAM 3 achieves state-of-the-art results, doubling performance over prior systems for PCS\non SA-Co in images and videos. We believe SAM 3 and the SA-Co benchmark will be important\nmilestones and pave the way for future research and applications in computer vision.\n9\n",
      "text_length": 4246,
      "elements": [
        {
          "type": "table",
          "bbox_1000": [
            145,
            63,
            825,
            167
          ],
          "bbox_pixels": [
            184,
            103,
            1051,
            275
          ],
          "label": "Table 6",
          "description": "SAM 3 improves over SAM 2 in VOS. †: Zero-shot. Shows performance metrics for various models (SAMURAI, SAM2Long, SeC, SAM 2.1 L, SAM 3) across datasets (MOSEv1, DAVIS17, LVOSv2, SA-V, YTVOs19, MOSEv2) with val/test splits. Includes avg. mIoU for SAM 3 on SA-37 benchmark.",
          "crop_path": "elements/p09_table_1_Table_6.png"
        },
        {
          "type": "table",
          "bbox_1000": [
            145,
            192,
            825,
            342
          ],
          "bbox_pixels": [
            184,
            316,
            1051,
            564
          ],
          "label": "Table 8",
          "description": "SAM 3 Agent results. Gray indicates fine-tuned results on ReasonSeg (train), * indicates reproduced results, underline indicates the main metric. †: LISA-13B-LLaVA1.5 for ReasonSeg; REAL for OmniLabel. Compares X-SAM, SegZero, RSVP, Overall SoTA Performance, and SAM 3 Agent variants (Qwen2.5-VL 7B, Llama4 Maverick, Qwen2.5-VL 72B, Gemini 2.5 Pro) on ReasonSeg (gIoU) and OmniLabel (AP) benchmarks.",
          "crop_path": "elements/p09_table_2_Table_8.png"
        },
        {
          "type": "table",
          "bbox_1000": [
            145,
            555,
            825,
            655
          ],
          "bbox_pixels": [
            184,
            915,
            1051,
            1080
          ],
          "label": "Table 9",
          "description": "Selected model and data ablations on SA-Co/Gold. Numbers across tables are not directly comparable. Shows ablation results for (a) Presence token, (b) Hard Negatives, (c) Training data, (d) SAM 3 + AI verifiers, with metrics CGF1, IL_MCC, pmF1 for various configurations.",
          "crop_path": "elements/p09_table_3_Table_9.png"
        }
      ],
      "extraction_time_seconds": 130.22,
      "total_page_time_seconds": 130.38
    },
    {
      "page_number": 26,
      "image": "page_26.png",
      "annotated_image": "page_26_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n1361\n1362\n1363\n1364\n1365\n1366\n1367\n1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377\n1378\n1379\n1380\n1381\n1382\n1383\n1384\n1385\n1386\n1387\n1388\n1389\n1390\n1391\n1392\n1393\n1394\n1395\n1396\n1397\n1398\n1399\n1400\n1401\n1402\n1403\nUnder review as a conference paper at ICLR 2026\nPL-Food\nSA-Co/SYN-Food\nSA-Co/HQ-Food\nTeacher\nBaseline\n1.5K\n30K\n150K\n750K\nNew domain data\n40\n45\n50\n55\n60\n65\nCGF1 (new domain)\n(a) Data scaling mixing pre-training data at a 1:1 ratio.\n1.5K\n7.5K\n30K\n150K\nNew domain data\n40\n45\n50\n55\n60\n65\nCGF1 (new domain)\n(b) Data scaling without mixing pre-training data.\nFigure 7: Domain adaptation via synthetic data. (a) SAM 3 + AI verifiers (teacher system) can annotate\nsynthetic (SYN) data in new domains (e.g., fine-grained food concepts) and achieve similar scaling behavior as\nwith human-annotated (HQ) data. (b) Not mixing in high-quality pre-training data can limit performance gains\nwhen fine-tuning on new domains, particularly when using synthetic data.\nWe study data scaling laws for three variants of “Food & drink” training data, evaluating performance\non the Wiki-Food&Drink subset of the SA-Co/Gold benchmark:\n• PL-Food: The target concepts are annotated using the data engine without AI verification. This\ndata is similar to typical pseudo-labelled data used in prior work for self-training (e.g. Minderer\net al. (2022)).\n• SA-Co/SYN-Food: This data is PL-Food, but cleaned by AI verifiers.\n• SA-Co/HQ-Food: Data annotated by human annotators.\nWe train the models in 2 steps to isolate the impact of the data from the new domain from other\ndata as well as to amortize training costs. We first pre-train a base model using SA-Co/HQ minus\nSA-Co/HQ-Food to establish base capability and a common starting point. Next, we fine-tune the\nsame base model with the three data variants in two settings: with or without mixing the pre-training\ndata.\nFig. 7a shows the scaling law with mixing the pre-training data in a 1:1 ratio. We observe some\nmodest improvement in performance with PL-Food compared to the baseline, but there is a large gap\nto the other variants due to its lower quality. SA-Co/HQ-Food and SA-Co/SYN-Food have similar\ndata scaling behavior, but a model trained using SA-Co/SYN-Food can eventually surpass using\nSA-Co/HQ-Food because SA-Co/SYN-Food can be scaled up without incurring any annotation cost.\nThe model trained on SA-Co/SYN-Food eventually gets very close to the performance of its teacher\nsystem.\nIn the second setting, we do not mix the pre-training data with the fine-tuning data from the new\ndomain. All three data variants result in worse performance in Fig. 7b (vs Fig. 7a). In this setting,\nthere is a larger gap between SA-Co/HQ-Food and SA-Co/SYN-Food reflecting the lower quality of\nSA-Co/SYN-Food (mainly lack of exhaustivity due to no human correction) but can eventually reach\na similar level of performance as using SA-Co/HQ-Food (likely due to volume and thereby improved\ncoverage). Comparing Fig. 7a and 7b, it is beneficial to include high-quality general-domain data\nwhen fine-tuning SAM 3 on new domains, particularly when using synthetic data, allowing us to\n“make up” for the lower quality in the synthetic data to some degree.\nB.4\nIMAGE DATA ENGINE ANNOTATION SPEED\nTab. 18 measures the speedup in the SAM 3 data engine from adding AI verifiers when collecting\ndata on a new domain with fine-grained concepts. We use the same setup as §B.3, annotating\nWiki-Food&Drink data generated with a data engine where neither SAM 3 nor AI verifiers have been\ntrained on Wiki-Food&Drink data. We annotate the same set of image-NP pairs in four settings:\n26\n",
      "text_length": 3660,
      "elements": [
        {
          "type": "figure",
          "bbox_1000": [
            130,
            93,
            807,
            260
          ],
          "bbox_pixels": [
            165,
            153,
            1028,
            429
          ],
          "label": "Figure 7",
          "description": "Two line charts comparing performance (CGF1 score) of different data variants (PL-Food, SA-Co/SYN-Food, SA-Co/HQ-Food, Teacher, Baseline) against increasing new domain data (1.5K to 750K). Chart (a) shows data scaling with mixing pre-training data at 1:1 ratio, while chart (b) shows data scaling without mixing pre-training data. Both charts show performance trends with synthetic and human-annotated data.",
          "crop_path": "elements/p26_figure_1_Figure_7.png"
        }
      ],
      "extraction_time_seconds": 63.81,
      "total_page_time_seconds": 63.93
    },
    {
      "page_number": 29,
      "image": "page_29.png",
      "annotated_image": "page_29_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "1512\n1513\n1514\n1515\n1516\n1517\n1518\n1519\n1520\n1521\n1522\n1523\n1524\n1525\n1526\n1527\n1528\n1529\n1530\n1531\n1532\n1533\n1534\n1535\n1536\n1537\n1538\n1539\n1540\n1541\n1542\n1543\n1544\n1545\n1546\n1547\n1548\n1549\n1550\n1551\n1552\n1553\n1554\n1555\n1556\n1557\n1558\n1559\n1560\n1561\n1562\n1563\n1564\n1565\nUnder review as a conference paper at ICLR 2026\nFigure 8: SAM 3 architecture. We highlight new components in yellow, SAM 2 (Ravi et al., 2024) in blue and\nPE (Bolya et al., 2025) in cyan.\n(feature embeddings) representing each frame to the dual-encoder consisting of the fusion encoder\nand memory attention, described below.\nGeometry and Exemplar Encoder. The geometry and exemplar encoder is primarily used to encode\nimage exemplars (if present) for the PCS task. It is additionally also used to encode visual prompts\nfor the PVS task on images but this is an auxiliary functionality that is primarily used to include\npre-training data for the PVS task in stages-2,-3 of training (see §D.4.1) and is thereby used to enable\na more modular training approach.\nEach individual image exemplar is encoded using positional embedding, label embedding (positive or\nnegative) and ROI-pooled visual features that are concatenated (comprising “exemplar tokens”) and\nprocessed by a small transformer. Visual prompts (points, boxes) for auxiliary training are encoded\nin a similar manner, comprising “geometry tokens”. It is possible for neither “geometry tokens” nor\n“exemplar tokens” to be present (e.g. when only a text prompt is used). The geometry or exemplar\ntokens attend to each other via self-attention and also cross-attend to the frame-embeddings of the\ncorresponding (unconditioned) frame from the image encoder.\nFusion Encoder. The text and geometry/exemplar tokens together constitute the prompt tokens. The\nfusion encoder accepts the unconditioned frame-embeddings and conditions on prompt tokens using\na stack of 6 transformer blocks with self- and cross-attention (to prompt tokens) layers followed by an\nMLP. We use vanilla self-attention operations. The output of the fusion encoder are the conditioned\nframe-embeddings.\nDecoder. The decoder architecture follows Carion et al. (2020); Kamath et al. (2021) as a starting\npoint and is a stack of 6 transformer blocks. K learned object queries (not to be confused with\nprompts) self-attend to each other and cross attend to the prompts tokens (made up of text and\ngeometry/exemplar tokens) and conditioned frame-embeddings, followed by an MLP. We use box-to-\npixel relative position bias (Lin et al., 2023) in the cross-attention layers attending to the conditioned\nframe-embeddings.\nFollowing standard practice in stronger DETR variants, we use iterative box refinement (Zhu et al.,\n2020), look-forward-twice (Zhang et al., 2022a) and hybrid matching (Jia et al., 2022) and Divide-\nAnd-Conquer (DAC) DETR (Hu et al., 2023). By default, we use K = 200 object queries. Bounding\nboxes and scores are predicted using dedicated MLPs and accept the object queries as input.\nPresence Head. Classifying each object in isolation is often difficult, due to insufficient information,\nand may require contextual information from the rest of the image. Forcing each object query to\nacquire such global awareness is however detrimental, and can conflict with the localization objectives\nthat are by nature very local. To address this, we propose decomposing the classification problem into\n29\n",
      "text_length": 3403,
      "elements": [
        {
          "type": "figure",
          "bbox_1000": [
            150,
            108,
            830,
            330
          ],
          "bbox_pixels": [
            191,
            178,
            1058,
            544
          ],
          "label": "Figure 8",
          "description": "SAM 3 architecture diagram showing components from Perception Encoder (cyan), SAM 2 (blue), and new components in SAM 3 (yellow). The diagram illustrates the flow from text and image inputs through encoders, detectors, trackers, memory banks, and decoders to produce output including masks, boxes, and scores.",
          "crop_path": "elements/p29_figure_1_Figure_8.png"
        }
      ],
      "extraction_time_seconds": 55.59,
      "total_page_time_seconds": 55.74
    },
    {
      "page_number": 30,
      "image": "page_30.png",
      "annotated_image": "page_30_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "1566\n1567\n1568\n1569\n1570\n1571\n1572\n1573\n1574\n1575\n1576\n1577\n1578\n1579\n1580\n1581\n1582\n1583\n1584\n1585\n1586\n1587\n1588\n1589\n1590\n1591\n1592\n1593\n1594\n1595\n1596\n1597\n1598\n1599\n1600\n1601\n1602\n1603\n1604\n1605\n1606\n1607\n1608\n1609\n1610\n1611\n1612\n1613\n1614\n1615\n1616\n1617\n1618\n1619\nUnder review as a conference paper at ICLR 2026\ntwo complementary components: a global-level classification that determines object presence within\nthe entire image, and a local-level localization that functions as foreground-background segmentation\nwhile preventing duplicate detections. Formally, we add the following probabilistic structure: instead\nof predicting p(queryi matches NP) directly, we break it down as follows:\np(queryi matches NP) = p(queryi matches NP | NP appears in image) · p(NP appears in image).\nTo compute p(NP appears in image), we use a presence token, which is added to our decoder and\nthen fed through an MLP classification head. Crucially, the presence score is shared by all object\nqueries. The per-query classification loss is kept as usual, but to account for the decomposition, we\nonly compute it when the NP is present in the image (see §B.1 for ablations on supervision strategy).\nThe same decomposition is applied to the semantic segmentation head, where we reuse the same\npresence score, and train the binary mask head only on the positive examples.\nBesides being more robust to false positives, decomposing the prediction in this manner is also more\nflexible, e.g. in typical counting tasks, we already know the NP is present in the image and instead want\nto know how many instances are present - in this case we can simply set p(NP is present in frame) =\n1. The presence token is concatenated with the object queries in all operations, but is excluded from\nDAC.\nWe also learn 4 geometric queries. Their function is similar to the 4 geometric queries in SAM\n1 and 2 (where they were called “output tokens”) and are used to perform the PVS on individual\nimage or video frames during the stags-2,-3 of training, see §D.4.1. The prompts are provided by the\n“geometry tokens” in the form of visual prompts. The presence score is set to 1 when performing the\nPVS task on a single frame, as the target is known to be present in the frame.\nSegmentation Head. The segmentation head is adapted from MaskFormer (Cheng et al., 2021).\nSemantic segmentation and instance segmentation share the same segmentation head. The conditioned\nfeatures from the fusion encoder are used to produce semantic segmentation masks, while instance\nsegmentation additionally uses the decoder’s output object queries. “Multi-scale” features are\nprovided to the segmentation head using SimpleFPN (Li et al., 2022d), since the vision encoder is a\n(single-scale) ViT.\nHandling Ambiguity. Experimentally, if we train a SAM 3 model without handling ambiguities as\ndescribed in §2 in any way, we observe that the model tends to predict several valid but conflicting\ninterpretations of the phrase. This is expected; if in our training dataset a given phrase has two\ndistinct interpretations, and roughly half the data is annotated assuming the first one, while the other\nhalf follows the second one, then the solution that minimizes the training loss is to output both\ninterpretations with 50% confidence. However, this behavior is undesirable for end-users, because it\nproduces conflicting, sometimes overlapping masks.\nTo address this issue, we add an ambiguity head to our model. Similar to SAM 1 and 2, this head is a\nmixture of experts, where we train in parallel K experts, and only supervise the expert that gets the\nlowest loss (winner-takes-all). We find that K = 2 performs the best and that it is more difficult to\ntrain K > 3 experts due to mode collapse.\nFor a mixture of K experts, each producing an output yk with loss Lk, the mixture loss is a weighted\naverage:\nLoss:\nLMoE =\nK\nX\nk=1\npk Lk\nGradient:\n∂LMoE\n∂θj\n= pj\n∂Lj\n∂θj\n.\nIn our winner-takes-all variant, only the expert with the lowest loss receives gradient:\nLoss:\nk⋆= arg min\nk\nLk,\nLWTA = Lk⋆\nGradient:\n∂LWTA\n∂θj\n=\n\n\n\n\n\n∂Lk⋆\n∂θj\n,\nif j = k⋆,\n0,\notherwise.\n30\n",
      "text_length": 4103,
      "elements": [
        {
          "type": "equation",
          "bbox_1000": [
            150,
            167,
            840,
            186
          ],
          "bbox_pixels": [
            191,
            275,
            1071,
            306
          ],
          "label": "Equation 1",
          "description": "LaTeX: $p(\\text{query}_i \\text{ matches NP}) = p(\\text{query}_i \\text{ matches NP} \\mid \\text{NP appears in image}) \\cdot p(\\text{NP appears in image})$",
          "latex": "$p(\\text{query}_i \\text{ matches NP}) = p(\\text{query}_i \\text{ matches NP} \\mid \\text{NP appears in image}) \\cdot p(\\text{NP appears in image})$",
          "crop_path": "elements/p30_equation_1_Equation_1.png",
          "rendered_path": "elements/p30_equation_1_Equation_1_rendered.png"
        },
        {
          "type": "equation",
          "bbox_1000": [
            250,
            750,
            710,
            782
          ],
          "bbox_pixels": [
            318,
            1237,
            905,
            1290
          ],
          "label": "Equation 2",
          "description": "LaTeX: $\\text{Loss:} \\quad \\mathcal{L}_{\\text{MoE}} = \\sum_{k=1}^{K} p_k \\mathcal{L}_k \\quad \\text{Gradient:} \\quad \\frac{\\partial \\mathcal{L}_{\\text{MoE}}}{\\partial \\theta_j} = p_j \\frac{\\partial \\mathcal{L}_j}{\\partial \\theta_j}$",
          "latex": "$\\text{Loss:} \\quad \\mathcal{L}_{\\text{MoE}} = \\sum_{k=1}^{K} p_k \\mathcal{L}_k \\quad \\text{Gradient:} \\quad \\frac{\\partial \\mathcal{L}_{\\text{MoE}}}{\\partial \\theta_j} = p_j \\frac{\\partial \\mathcal{L}_j}{\\partial \\theta_j}$",
          "crop_path": "elements/p30_equation_2_Equation_2.png",
          "rendered_path": "elements/p30_equation_2_Equation_2_rendered.png"
        },
        {
          "type": "equation",
          "bbox_1000": [
            250,
            820,
            650,
            902
          ],
          "bbox_pixels": [
            318,
            1353,
            828,
            1488
          ],
          "label": "Equation 3",
          "description": "LaTeX: $\\text{Loss:} \\quad k^* = \\arg\\min_k \\mathcal{L}_k, \\quad \\mathcal{L}_{\\text{WTA}} = \\mathcal{L}_{k^*} \\quad \\text{Gradient:} \\quad \\frac{\\partial \\mathcal{L}_{\\text{WTA}}}{\\partial \\theta_j} = \\begin{cases} \\frac{\\partial \\mathcal{L}_{k^*}}{\\partial \\theta_j}, & \\text{if } j = k^*, \\\\ 0, & \\text{otherwise}. \\end{cases}$",
          "latex": "\\begin{align*}\n\\text{Loss:} \\quad & k^* = \\arg\\min_k \\mathcal{L}_k, \\quad \\mathcal{L}_{\\text{WTA}} = \\mathcal{L}_{k^*} \\\\\n\\text{Gradient:} \\quad & \\frac{\\partial \\mathcal{L}_{\\text{WTA}}}{\\partial \\theta_j} = \\begin{cases} \\frac{\\partial \\mathcal{L}_{k^*}}{\\partial \\theta_j}, & \\text{if } j = k^*, \\\\ 0, & \\text{otherwise}. \\end{cases}\n\\end{align*}",
          "crop_path": "elements/p30_equation_3_Equation_3.png",
          "rendered_path": "elements/p30_equation_3_Equation_3_rendered.png"
        }
      ],
      "extraction_time_seconds": 133.33,
      "total_page_time_seconds": 133.69
    }
  ],
  "timing": {}
}