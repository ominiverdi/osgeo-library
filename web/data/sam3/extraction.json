{
  "document": "sam3",
  "source_file": "sam3.pdf",
  "extraction_date": "2025-12-16T17:37:15.322414",
  "model": "qwen3-vl-32b",
  "pages": [
    {
      "page_number": 1,
      "image": "page_01.png",
      "annotated_image": "page_01_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "000\n001\n002\n003\n004\n005\n006\n007\n008\n009\n010\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n050\n051\n052\n053\nUnder review as a conference paper at ICLR 2026\nSAM 3: SEGMENT ANYTHING WITH CONCEPTS\nAnonymous authors\nPaper under double-blind review\nABSTRACT\nWe present Segment Anything Model (SAM) 3, a unified model that detects, seg-\nments, and tracks objects in images and videos based on concept prompts, which\nwe define as either short noun phrases (e.g., “yellow school bus”), image exemplars,\nor a combination of both. Promptable Concept Segmentation (PCS) takes such\nprompts and returns segmentation masks and unique identities for all matching\nobject instances. To advance PCS, we build a scalable data engine that produces\na high-quality dataset with 4M unique concept labels, including hard negatives,\nacross images and videos. Our model consists of a vision backbone shared be-\ntween an image-level detector and a memory-based video tracker. Recognition\nand localization are decoupled with a presence head, which significantly boosts\ndetection accuracy. SAM 3 delivers a 2× gain over existing systems in both image\nand video PCS, and improves previous SAM capabilities in interactive visual seg-\nmentation tasks. We open source SAM 3 along with our new Segment Anything\nwith Concepts (SA-Co) benchmark.\n1\nINTRODUCTION\nThe ability to find and segment anything in a visual scene is foundational for multimodal AI, powering\napplications in robotics, content creation, augmented reality, data annotation, and scientific fields.\nThe SAM series (Kirillov et al., 2023; Ravi et al., 2024) introduced the promptable segmentation task\nto segment objects in images and videos via interactive prompts, including visual inputs like points,\nboxes, and masks marking a specific object, or text inputs describing an object. However, SAM 1\nand SAM 2 focus on visual prompts and segment a single object instance per prompt. While these\nmethods achieved a breakthrough for this critical task, they did not address the broader task of finding\nand segmenting all instances of a concept appearing anywhere in the input (e.g., all “cats” in a video).\nIn this work, we present SAM 3, a model that achieves a step change in promptable segmentation in\nimages and videos, improving Promptable Visual Segmentation (PVS) relative to SAM 2 and setting\na new standard for Promptable Concept Segmentation (PCS). We formalize the PCS task as taking\ntext and/or image exemplars as input, and predicting instance and semantic masks for every single\nobject matching the concept, while preserving object identities across video frames (§2). We focus\non recognizing atomic visual concepts and thus constrain text to simple noun phrases (NPs), such as\n“red apple” or “striped cat”. Example outputs are shown in Fig. 1.\nFigure 1: SAM 3 improves over SAM 2 on promptable visual segmentation with clicks (left) while advancing\npromptable concept segmentation (right) where users can segment all instances of a visual concept specified by\na short noun phrase, image exemplars, or a combination of both.\n1\n",
      "text_length": 3178,
      "elements": [
        {
          "type": "figure",
          "bbox_1000": [
            199,
            721,
            805,
            865
          ],
          "bbox_pixels": [
            253,
            1189,
            1026,
            1427
          ],
          "label": "Figure 1",
          "description": "A comparative illustration showing SAM 3's improvements over SAM 2. Left side shows 'Promptable Visual Segmentation' with image and video examples using positive/negative points as prompts. Right side shows 'Promptable Concept Segmentation' with examples like 'a striped cat', 'a round cell', 'small window', 'a kangaroo', and 'hard hat', using noun phrases and/or image exemplars as prompts. The figure highlights the shift from segmenting single objects via clicks to segmenting all instances of a concept via text or image prompts.",
          "crop_path": "elements/p01_figure_1_Figure_1.png"
        }
      ]
    },
    {
      "page_number": 2,
      "image": "page_02.png",
      "annotated_image": "page_02_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "054\n055\n056\n057\n058\n059\n060\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n100\n101\n102\n103\n104\n105\n106\n107\nUnder review as a conference paper at ICLR 2026\nFigure 2: Illustration of supported initial and optional interactive refinement prompts in the PCS task.\nWhile SAM 3 is not designed for long referring language expressions or queries requiring reasoning,\nwe show that it can be straightforwardly combined with a Multimodal Large Language Model\n(MLLM) to handle more complex language prompts.\nThe PCS task is inherently ambiguous due to its open-vocabulary nature, with many concepts having\nmultiple interpretations; e.g., “small window” is subjective (small vs large) and has ambiguous\nboundaries (with shutters vs without). Our approach systematically accounts for these ambiguities at\nevery stage, including data collection, metric design, and modeling. Consistent with previous SAM\nversions, SAM 3 is fully interactive, allowing users to resolve ambiguities by adding refinement\nprompts to guide the model towards their intended output.\nOur model (§3) consists of a detector and a tracker that share a vision encoder (Bolya et al., 2025).\nThe detector is a DETR-based (Carion et al., 2020) model conditioned on text, geometry, and image\nexemplars. To address the challenge of open-vocabulary concept detection, we introduce a separate\npresence head to decouple recognition and localization, which is especially effective when training\nwith challenging negative phrases. The tracker inherits the SAM 2 transformer encoder-decoder\narchitecture, supporting video segmentation and interactive refinement. The decoupled design for\ndetection and tracking avoids task conflict, as the detector needs to be identity agnostic, while the\ntracker’s main objective is to separate identities in the video.\nTo unlock major performance gains, we build a scalable human- and model-in-the-loop data en-\ngine (§4) that annotates a large and diverse training dataset. We innovate upon prior data engine\ndesigns in three key ways: i) media curation: we curate more diverse media domains than past\napproaches that rely on homogeneous web sources, ii) label curation: we significantly increase\nlabel diversity and difficulty by leveraging an ontology and multimodal LLMs as “AI annotators” to\ngenerate noun phrases and hard negatives, iii) label verification: we double annotation throughput by\nfine-tuning MLLMs to be effective “AI verifiers” that achieve near-human performance. Starting from\nnoisy media-phrase-mask pseudolabels, our data engine checks mask quality and exhaustivity using\nboth human and AI verifiers, filtering out correctly labeled examples and identifying challenging\nerror cases. Human annotators then focus on fixing these errors by manually correcting masks. This\nenables us to annotate high-quality training data with 4M unique phrases and 52M masks, and a\nsynthetic dataset with 38M phrases and 1.4B masks. We additionally create the Segment Anything\nwith Concepts (SA-Co) benchmark for PCS (§5) containing 214K unique concepts with exhaustive\nmasks in 124K images and 1.7K videos, > 50× more concepts than existing benchmarks.\nOur experiments (§6) show that SAM 3 sets a new state-of-the-art in promptable segmentation, e.g.,\nreaching a zero-shot mask AP of 47.0 on LVIS vs the current best of 38.5, surpassing baselines\non our new SA-Co benchmark by at least 2×, and improving upon SAM 2 on PVS benchmarks.\nAblations (§B) verify that the choice of backbone, novel presence head, and adding hard negatives\nall boost results, and establish scaling laws on the PCS task for both our high-quality and synthetic\ndatasets. We open-source the SA-Co benchmark and release the SAM 3 checkpoints and inference\ncode. On an H200 GPU, SAM 3 runs in 30 ms for a single image with 100+ detected objects. In\nvideo, the inference latency scales with the number of objects, sustaining near real-time performance\nfor ∼5 concurrent objects. Next we dive into the task, and review related work in §A.\n2\nPROMPTABLE CONCEPT SEGMENTATION (PCS)\nWe define the Promptable Concept Segmentation task as follows: given an image or short video (≤30\nsecs), detect, segment and track all instances of a visual concept specified by a short text phrase,\nimage exemplars, or a combination of both. We restrict concepts to those defined by simple noun\nphrases (NPs) consisting of a noun and optional modifiers. Noun-phrase prompts (when provided)\nare global to all frames of the image/video, while image exemplars can be provided on individual\nframes as positive or negative bounding boxes to iteratively refine the target masks (see Fig. 2).\n2\n",
      "text_length": 4732,
      "elements": [
        {
          "type": "figure",
          "bbox_1000": [
            225,
            98,
            778,
            189
          ],
          "bbox_pixels": [
            286,
            161,
            991,
            311
          ],
          "label": "Figure 2",
          "description": "Illustration of supported initial and optional interactive refinement prompts in the PCS task, showing four stages: initial prompt with text and box, output masks, refinement visual prompts with positive/negative examples, and refined masks.",
          "crop_path": "elements/p02_figure_1_Figure_2.png"
        }
      ]
    },
    {
      "page_number": 3,
      "image": "page_03.png",
      "annotated_image": "page_03_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\nUnder review as a conference paper at ICLR 2026\nFigure 3: SAM 3 architecture overview. See Fig. 8 for a more detailed diagram.\nAll prompts must be consistent in their category definition, or the model’s behavior is undefined; e.g.,\n“fish” cannot be refined with subsequent exemplar prompts of just the tail; instead the text prompt\nshould be updated. Exemplar prompts are particularly useful when the model initially misses some\ninstances, or when the concept is rare.\nOur vocabulary includes any simple noun phrase groundable in a visual scene, which makes the task\nintrinsically ambiguous. There can be multiple interpretations of phrases arising from polysemy\n(“mouse” device vs. animal), subjective descriptors (“cozy”, “large”), vague or context-dependent\nphrases that may not even be groundable (“brand identity”), boundary ambiguity (whether “mirror”\nincludes the frame), and factors like occlusion and blur that obscure object extent. While similar issues\nappear in large closed-vocabulary corpora (e.g., LVIS (Gupta et al., 2019)), they are alleviated by\ncarefully curating the vocabulary and setting a clear definition of all the classes of interest. We address\nthe ambiguity problem by collecting test annotations from three experts, adapting the evaluation\nprotocol to allow multiple valid interpretations (§F.3), designing the data pipeline and guidelines to\nminimize ambiguity during annotation, and introducing an ambiguity module in the model (§D.2).\n3\nMODEL\nSAM 3 is a generalization of SAM 2, supporting the new PCS task (§2) as well as the PVS task. It\ntakes concept prompts (simple noun phrases, image exemplars) or visual prompts (points, boxes,\nmasks) to define the objects to be (individually) segmented spatio-temporally. Image exemplars\nand visual prompts can be iteratively added on individual frames to refine the target masks—false\npositive and false negative objects can be removed or added respectively using image exemplars and\nan individual mask(let) can be refined using PVS in the style of SAM 2. Our architecture is broadly\nbased on the SAM and (M)DETR (Carion et al., 2020; Kamath et al., 2021) series. Fig. 3 shows the\nSAM 3 architecture, consisting of a dual encoder-decoder transformer—a detector for image-level\ncapabilities—which is used in combination with a tracker and memory for video. The detector and\ntracker ingest vision-language inputs from an aligned Perception Encoder (PE) backbone (Bolya\net al., 2025). We present an overview below, see §D for details.\nDetector Architecture. The architecture of the detector follows the general DETR paradigm. The\nimage and text prompt are first encoded by PE and image exemplars, if present, are encoded by\nan exemplar encoder. We refer to the image exemplar tokens and text tokens jointly as “prompt\ntokens”. The fusion encoder then accepts the unconditioned embeddings from the image encoder and\nconditions them by cross-attending to the prompt tokens. The fusion is followed by a DETR-like\ndecoder, where learned object queries cross-attend to the conditioned image embeddings from the\nfusion encoder. Each layer, for each object query, predicts a classification logit (in our case, a binary\nlabel of whether the object corresponds to the prompt), and a delta from the bounding box predicted\nby the previous level, following Zhu et al. (2020). We use box-region-positional bias (Lin et al., 2023)\nto help focalize the attention on each object, but unlike recent DETR models, we stick to vanilla\nattention. During training, we adopt dual supervision from DAC-DETR (Hu et al., 2023), and the\nAlign loss (Cai et al., 2024). The mask head is adapted from MaskFormer (Cheng et al., 2021). In\naddition, we also have a semantic segmentation head, which predicts a binary label for every pixel in\nthe image, indicating whether or not it corresponds to the prompt. See §D for details.\nPresence Token. It can be difficult for each of the proposal queries to both recognize (what) and\nlocalize (where) an object in the image/frame. For the recognition component, contextual cues from\nthe entire image are important. However, forcing proposal queries to understand the global context\ncan be counterproductive, as it conflicts with the inherently local nature of the localization objective.\n3\n",
      "text_length": 4488,
      "elements": [
        {
          "type": "figure",
          "bbox_1000": [
            112,
            99,
            802,
            238
          ],
          "bbox_pixels": [
            142,
            163,
            1022,
            392
          ],
          "label": "Figure 3",
          "description": "SAM 3 architecture overview diagram showing components like Text Encoder, Image Encoder, Detector, Tracker, Memory Bank, and their interactions for processing prompts and generating masks across frames. Includes visual elements such as exemplar images, masks, and color-coded data flow.",
          "crop_path": "elements/p03_figure_1_Figure_3.png"
        }
      ]
    },
    {
      "page_number": 5,
      "image": "page_05.png",
      "annotated_image": "page_05_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\nUnder review as a conference paper at ICLR 2026\nFigure 4: Overview of the final SAM 3 data engine. See §F.1 for details of collected data.\nTraining Stages.\nWe train SAM 3 in four stages that progressively add data and capabilities:\n1) Perception Encoder (PE) pre-training, 2) detector pre-training, 3) detector fine-tuning, and 4) tracker\ntraining with a frozen backbone. See §D.4.1 for details.\n4\nDATA ENGINE\nAchieving a step change on PCS with SAM 3 requires training on a large and diverse set of concepts\nand visual domains, beyond any existing dataset (see Figure 10). Our solution is to build an efficient\ndata engine that iteratively generates annotated data via a feedback loop with SAM 3, human\nannotators, and AI annotators, actively mining media-phrase pairs on which the current version of\nSAM 3 fails to produce high-quality training data to further improve the model. By delegating certain\ntasks to AI annotators—models that match or surpass human accuracy—we more than double the\nthroughput compared to a human-only annotation pipeline. We develop the data engine in four phases,\nwith each phase increasing the use of AI models to steer human effort to the most challenging failure\ncases, alongside expanding visual domain coverage. Phases 1-3 focus only on images, with Phase 4\nexpanding to videos. We describe the key steps here; details and metrics are in §E.\nData Engine Components (Fig. 4). Media inputs (image or video) are mined from a large pool\nwith the help of a curated ontology. An AI model proposes noun phrases (NPs) describing visual\nconcepts, followed by another model (e.g., SAM 3) that generates candidate instance masks for each\nproposed NP. The proposed masks are verified by a two-step process: first, in Mask Verification\n(MV) annotators accept or reject masks based on their quality and relevance to the NP. Second, in\nExhaustivity Verification (EV) annotators check if all instances of the NP have been masked in the\ninput. Any media-NP pairs that did not pass the exhaustivity check are sent to a manual correction\nstage, where humans add, remove or edit masks (using SAM 1 in a browser based tool), or use “group”\nmasks for small, hard to separate objects. Annotators may reject ungroundable or ambiguous phrases.\nPhase 1: Human Verification. At first, data mining is done by randomly sampling images and NP\nproposal is done with a simple captioner and parser. The initial mask proposal model is SAM 2\nprompted with the output of an off-the-shelf open-vocabulary detector, and verifiers are all human.\nThe collected 4.3M image-NP pairs form the initial SA-Co/HQ dataset. We train SAM 3 on this data\nand use it as the mask proposal model for the next phase.\nPhase 2: Human + AI Verification. In this next phase, we use human accept/reject labels from the\nMV and EV tasks collected in Phase 1 to fine-tune Llama 3.2 (Dubey et al., 2024) to create AI verifiers\nto automatically perform the MV and EV tasks. These models receive image-phrase-mask triplets\nand produce multiple-choice judgements of mask quality or exhaustivity. This new auto-verification\nprocess allows human effort to be focused on the most challenging cases. We continue to re-train\nSAM 3 on newly collected data and update it 6 times. As SAM 3 and AI verifiers improve, a higher\nproportion of labels are auto-generated, further accelerating data collection. The introduction of AI\nverifiers for MV and EV doubles the data engine’s throughput. We further upgrade the NP proposal\nstep to a Llama-based pipeline that also proposes hard negative NPs adversarial to SAM 3. This\nphase adds 122M image-NP pairs to SA-Co/HQ.\nPhase 3: Scaling and Domain Expansion. In this phase, we use AI models to mine increasingly\nchallenging cases and broaden domain coverage in SA-Co/HQ to 15 datasets (Fig. 11). In new\ndomains, the MV AI verifier performs well zero-shot, while the EV AI verifier improves with modest\ndomain-specific human supervision. We also expand concept coverage to long-tail, fine-grained\nconcepts by extracting NPs from each image’s alt-text where available and by mining concepts from\na 22.4M node SA-Co ontology based on Wikidata (17 top-level categories, 72 sub-categories). We\niterate SAM 3 training 7 times and AI verifiers 3 times, and add 19.5M image-NP pairs to SA-Co/HQ.\n5\n",
      "text_length": 4510,
      "elements": [
        {
          "type": "figure",
          "bbox_1000": [
            225,
            100,
            925,
            214
          ],
          "bbox_pixels": [
            286,
            165,
            1179,
            353
          ],
          "label": "Figure 4",
          "description": "Overview of the final SAM 3 data engine, showing a flowchart with components such as Media Pool, Mine data, Propose NPs, Propose mask (SAM 3), AI verifiers, Human verifiers, and final annotations. Includes icons for data, AI model, human, label, and train, with arrows indicating the data flow and feedback loop.",
          "crop_path": "elements/p05_figure_1_Figure_4.png"
        }
      ]
    },
    {
      "page_number": 6,
      "image": "page_06.png",
      "annotated_image": "page_06_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\nUnder review as a conference paper at ICLR 2026\nFigure 5: Example video (top) and images (bottom) from SA-Co with annotated phrases and instance masks/IDs.\nPhase 4: Video Annotation. This phase extends the data engine to video. We use a mature SAM 3 to\ncollect targeted quality annotations that capture video-specific challenges. The data mining pipeline\napplies scene/motion filters, content balancing, ranking, and targeted searches. Video frames are\nsampled (randomly or by object density) and sent to the image annotation flow (from phase 3).\nMasklets are produced with SAM 3 (now extended to video) and post-processed via deduplication\nand removal of trivial masks. Because video annotation is more difficult, we concentrate humans on\nlikely failures by favoring clips with many crowded objects and tracking failures. The collected video\ndata SA-Co/VIDEO consists of 52.5K videos and 467K masklets. See §E.6 for details.\n5\nSEGMENT ANYTHING WITH CONCEPTS (SA-CO) DATASET\nTraining Data. We collect three image datasets for the PCS task: i) SA-Co/HQ, the high-quality\nimage data collected from the data engine in phases 1-4, ii) SA-Co/SYN, a synthetic dataset of images\nlabeled by a mature version of the data engine (phase 3) without any human involvement, and iii)\nSA-Co/EXT, fifteen external datasets which have instance mask annotations, enriched with hard\nnegatives using our SA-Co ontology pipeline. Notably in the SA-Co/HQ dataset we annotate 5.2M\nimages and 4M unique NPs, making it the largest high-quality open-vocab segmentation dataset.\nWe also annotate a video dataset, SA-Co/VIDEO, containing 52.5K videos and 24.8K unique NPs,\nforming 134K video-NP pairs. The videos have an average length of 84.1 frames at 6 fps. See §F.1\nfor details including full statistics, comparison with existing datasets and the distribution of concepts.\nSA-Co Benchmark. The SA-Co evaluation benchmark has all together 214K unique phrases, 126K\nimages and videos, and over 3M media-phrase pairs with challenging hard negative labels to test\nopen-vocabulary recognition. It consists of several splits: SA-Co/Gold has seven domains and\neach image-NP pair is annotated by three different annotators (used measure human performance);\nSA-Co/Silver has ten domains and only one human annotation per image-NP pair; SA-Co/Bronze\nand SA-Co/Bio are nine existing datasets either with existing mask annotations or masks generated\nby using boxes as prompts to SAM 2. The SA-Co/VEval benchmark has three domains and one\nannotator per video-NP pair. See Tab. 29 for dataset statistics and Fig. 5 for example annotations.\nMetrics. We aim to measure the usefulness of the model in downstream applications. Detection\nmetrics such as average precision (AP) do not account for calibration, which means that models\ncan be difficult to use in practice. To remedy this, we only evaluate predictions with confidence\nabove 0.5, effectively introducing a threshold that mimics downstream usages and enforces good\ncalibration. The PCS task can be naturally split into two sub-tasks, localization and classification.\nWe evaluate localization using positive macro F1 (pmF1) on positive media-phrase pairs with at least\none ground-truth and one predicted mask. Classification is measured with image-level Matthews\nCorrelation Coefficient (IL MCC) which ranges in [−1, 1] and evaluates binary prediction at the\nimage level (“is the object present or not?”) without regard for the quality of the masks. Our main\nmetric, classification-gated F1 (CGF1), combines these as follows: CGF1 = 100∗pmF1 ∗IL MCC.\nFull definitions are in §F.3.\n6\n",
      "text_length": 3815,
      "elements": [
        {
          "type": "figure",
          "bbox_1000": [
            210,
            93,
            788,
            331
          ],
          "bbox_pixels": [
            267,
            153,
            1004,
            546
          ],
          "label": "Figure 5",
          "description": "Example video (top) and images (bottom) from SA-Co with annotated phrases and instance masks/IDs. The top row shows four video frames with color-coded object masks and corresponding labels (e.g., 'a tree', 'the fabric', 'a yellow shirt'). The bottom row displays three images with similar annotations, including items like 'plastic bag', 'pomegranate', 'a chain', 'a MacBook', and 'white iPhone', each with color-coded masks and labels.",
          "crop_path": "elements/p06_figure_1_Figure_5.png"
        }
      ]
    },
    {
      "page_number": 7,
      "image": "page_07.png",
      "annotated_image": "page_07_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\nUnder review as a conference paper at ICLR 2026\nInstance Segmentation\nBox Detection\nSemantic Segmentation\nLVIS\nSA-Co\nLVIS\nCOCO\nSA-Co\nADE-847 PC-59 Cityscapes\nModel\nCGF1 AP\nGold Silver Bronze Bio\nCGF1\nAP\nAP\nAPo\nGold Silver Bronze Bio\nmIoU\nmIoU\nmIoU\nHumanmin\n–\n–\n74.2\n–\n–\n–\n–\n–\n–\n–\n76.2\n–\n–\n–\n–\n–\n–\nHumanmax\n–\n–\n81.4\n–\n–\n–\n–\n–\n–\n–\n83.6\n–\n–\n–\n–\n–\n–\nOWLv2\n35.5\n–\n22.9\n12.1\n8.5\n1.1\n35.2\n35.2\n38.2\n42.4\n23.3\n11.7\n8.5\n1.6\n–\n–\n–\nOWLv2⋆\n45.7\n43.4\n34.3\n19.3\n19.3\n0.2\n47.4\n45.5\n46.1\n23.9\n35.4\n19.2\n19.4\n0.3\n–\n–\n–\ngDino-T\n32.9\n–\n9.1\n7.4\n11.1\n0.6\n33.8\n20.5\n45.7\n35.3\n9.4\n6.8\n12.3\n0.7\n–\n–\n–\nLLMDet-L 48.1\n36.3\n12.9\n12.1\n18.8\n0.5\n53.7\n42.0\n55.6\n49.8\n13.5\n11.5\n20.6\n0.6\n–\n–\n–\nAPE-D⋆\n–\n53.0† 27.3\n15.0\n19.7\n0.0\n–\n59.6† 58.3†\n–\n29.4\n15.9\n21.8\n0.0\n9.2†\n58.5†\n44.2†\nDINO-X\n–\n38.5† 27.7δ\n–\n–\n–\n–\n52.4† 56.0†\n–\n29.4δ\n–\n–\n–\n–\n–\n–\nGemini 2.5 19.8\n–\n16.4\n10.5\n8.9\n10.2\n23.7\n–\n–\n–\n18.7\n12.2\n10.1\n12.0\n–\n–\n–\nSAM 3\n52.8\n47.0\n65.0\n57.1\n49.5\n59.3\n57.5\n51.7\n53.5\n55.5\n67.7\n58.0\n53.1\n60.0\n14.7\n59.4\n65.1\nTable 1: Evaluation on image concept segmentation with text. APo corresponds to COCO-O accuracy, ⋆partially\ntrained on LVIS. †from original papers, δfrom DINO-X API. Gray numbers indicate usage of respective closed\nset training data (LVIS/COCO). Upper and lower bound for human performance given, see §F.4 for details.\nHandling Ambiguity. We collect 3 annotations per NP on SA-Co/Gold. We measure oracle accuracy\ncomparing each prediction to all ground truths and selecting the best score. See §F.3.\n6\nEXPERIMENTS\nWe evaluate SAM 3 across image/video segmentation, few-shot adaptation to detection and counting\nbenchmarks, and segmenting from complex language queries with SAM 3 + MLLM. We also show a\nsubset of ablations, with more in §B. References, more results and details are in §G.\nImage PCS with Text. We evaluate instance segmentation, box detection, and semantic segmentation\non external and our benchmarks. SAM 3 is prompted with a single NP at a time, and predicts instance\nmasks, bounding boxes, or semantic masks. As baselines, we evaluate OWLv2, GroundingDino, and\nLLMDet on box detection, and prompt SAM 1 with their boxes to evaluate segmentation. We also\ncompare to APE, DINO-X, and Gemini 2.5 Flash, a generalist LLM. Tab. 1 shows that zero-shot,\nSAM 3 is competitive on closed-vocabulary COCO, COCO-O and on LVIS boxes, and is significantly\nbetter on LVIS masks. On open-vocabulary SA-Co/Gold SAM 3 achieves double the CGF1 score of\nthe strongest baseline OWLv2⋆, and 88% of the estimated lower bound on human performance. The\nimprovements are even higher on the other SA-Co splits. Open vocabulary semantic segmentation\nresults on ADE-847, PascalConcept-59, and Cityscapes show that SAM 3 outperforms APE, a strong\nspecialist baseline. See §G.1 for details.\nODinW13\nRF-100VL\nModel\nAP0\nAP10 AP0\nAP10\nGemini2.5-Pro\n33.7\n–\n11.6\n9.8\ngDino-T\n49.7\n–\n15.7\n33.7\ngDino1.5-Pro\n58.7\n67.9\n–\n–\nSAM 3\n59.9\n71.6\n14.3\n35.7\nTable 2: Zero-shot and 10-shot\ntransfer on in-the-wild datasets.\nCOCO\nLVIS\nODinW13\nAP\nAP+\nAP+\nAP+\nAP\nAP+\nAP+\nAP+\nAP\nAP+\nAP+\nAP+\nModel\nT\nT\nI\nT+I\nT\nT\nI\nT+I\nT\nT\nI\nT+I\nT-Rex2\n52.2\n–\n58.5\n–\n45.8\n–\n65.8\n–\n50.3\n–\n61.8\n–\nSAM 3\n53.5\n56.8\n75.7\n76.0\n51.7\n53.4\n75.5\n77.0\n59.9\n62.5\n81.9\n79.6\nTable 3: Prompting with 1 exemplar on COCO, LVIS and ODinW35. Eval-\nuation per prompt type: T (text-only), I (image-only), and T+I (combined\ntext and image). AP+ is evaluated only on positives examples.\nFew-Shot Adaptation. We evaluate zero- and few-shot transfer of SAM 3 on ODinW13 and\nRoboflow100-VL, with their original labels as prompts. We do not perform any prompt tuning.\nWe fine-tune SAM 3 without mask loss, and report average bbox mAP in Tab. 2. SAM 3 achieves\nSoTA 10-shot performance, surpassing in-context prompting in Gemini and object detection experts\n(gDino); more details in §G.3. RF-100VL contains domains with specialized prompts that are out of\nSAM 3’s current scope, but SAM 3 adapts through fine-tuning more efficiently than baselines.\nPCS with 1 Exemplar. We first evaluate image exemplars using a single input box sampled at\nrandom from the ground truth. This can be done only on “positive” data, where each prompted object\nappears in the image. We report the corresponding AP+ in Tab. 3 across three settings: text prompt\n(T), exemplar image (I), and both text and image (T+I); SAM 3 outperforms prior state-of-the-art\nT-Rex2 by a healthy margin on COCO (+17.2), LVIS (+9.7), and ODinW (+20.1). See §G.2 for more\ndetails and results on SA-Co/Gold.\n7\n",
      "text_length": 4671,
      "elements": [
        {
          "type": "table",
          "bbox_1000": [
            169,
            100,
            835,
            248
          ],
          "bbox_pixels": [
            215,
            165,
            1064,
            409
          ],
          "label": "Table 1",
          "description": "Table 1: Evaluation on image segmentation with text. Contains performance metrics (CGF1, AP, Gold, Silver, Bronze, Bio, etc.) for various models (Human_min, Human_max, OWLv2, OWLv2*, gDino-T, LLMdet-L, APE-D*, DINO-X, Gemini 2.5, SAM 3) across tasks like Instance Segmentation (LVIS, SA-Co), Box Detection (LVIS, COCO, SA-Co), and Semantic Segmentation (ADE-847, PC-59, Cityscapes). Includes notes on oracle accuracy, training data usage, and human performance bounds.",
          "crop_path": "elements/p07_table_1_Table_1.png"
        },
        {
          "type": "table",
          "bbox_1000": [
            169,
            620,
            375,
            688
          ],
          "bbox_pixels": [
            215,
            1023,
            478,
            1135
          ],
          "label": "Table 2",
          "description": "Table 2: Zero-shot and 10-shot transfer on in-the-wild datasets. Shows AP0 and AP10 scores for models (Gemini2.5-Pro, gDino-T, gDino1.5-Pro, SAM 3) on ODinW13 and RF-100VL benchmarks.",
          "crop_path": "elements/p07_table_2_Table_2.png"
        },
        {
          "type": "table",
          "bbox_1000": [
            385,
            620,
            835,
            688
          ],
          "bbox_pixels": [
            490,
            1023,
            1064,
            1135
          ],
          "label": "Table 3",
          "description": "Table 3: Prompting with 1 exemplar on COCO, LVIS, and ODinW35. Evaluates AP and AP+ scores for models (T-Rex2, SAM 3) under different prompt types: T (text-only), I (image-only), T+I (combined text and image). AP+ is evaluated only on positive examples.",
          "crop_path": "elements/p07_table_3_Table_3.png"
        }
      ]
    },
    {
      "page_number": 8,
      "image": "page_08.png",
      "annotated_image": "page_08_annotated.png",
      "width": 1275,
      "height": 1650,
      "text": "378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\nUnder review as a conference paper at ICLR 2026\n0\n1\n2\n3\n4\n5\n6\n7\nNumber of Prompts\n65\n70\n75\n80\n85\nCGF1 (SA-Co/Gold)\nSAM 3 PCS\nPerfect SAM 3 PVS\nHybrid: 2 prompts PCS then PVS\nHybrid: 4 prompts PCS then PVS\nFigure 6: SAM 3’s interactive exemplar\nprompts vs the ideal PVS baseline on SA-\nCo. We report CGF1 vs # of box prompts,\naveraged over all SA-Co/Gold phrases.\nPCS with K Exemplars. Next, we evaluate SAM 3 in\nan interactive setting, simulating collaboration with a hu-\nman annotator. Starting with a text prompt, we iteratively\nadd one exemplar prompt at a time: missed ground truths\nare candidate positive prompts, false positive detections\nare candidate negative prompts. Results (Fig. 6) are com-\npared to a perfect PVS baseline, where we simulate the\nuser manually fixing errors using ideal box-to-mask cor-\nrections. SAM 3’s PCS improves CGF1 more quickly, as\nit generalizes from exemplars (e.g., detecting or suppress-\ning similar objects), while PVS only corrects individual\ninstances. After 3 clicks, interactive PCS outperforms\ntext-only by +18.6 CGF1 points and PVS refinement by\n+9.7. Performance plateaus after 4 clicks, as exemplars\ncannot fix poor-quality masks. Simulating a hybrid switch\nto PVS at this point yields further gains, showing that the\ntwo approaches are complementary.\nCountBench\nPixMo-Count\nModel\nMAE ↓\nAcc ↑\nMAE ↓\nAcc ↑\nDINO-X\n0.62\n82.9\n0.21\n85.0\nQwen2-VL-72B\n0.28\n86.7\n0.61\n63.7\nMolmo-72B\n0.27\n92.4\n0.17\n88.8\nGemini 2.5 Pro\n0.24\n92.4\n0.38\n78.2\nSAM 3\n0.11\n95.6\n0.22\n87.3\nTable 4: Accuracy on counting benchmarks.\nGray indicates usage of training sets.\nObject Counting. We evaluate on object counting bench-\nmarks CountBench and PixMo-Count to compare with\nseveral MLLMs using Accuracy (%) and Mean Absolute\nError (MAE) from previous technical reports and our own\nevaluations. See Tab. 4 for results and §G.4 for more eval-\nuation details. Compared to MLLMs, SAM 3 not only\nachieves good object counting accuracy, but also provides\nobject segmentation that most MLLMs cannot provide.\nVideo PCS with Text. We evaluate video segmentation with text prompts on both our SA-Co/VEval\nbenchmark and existing public benchmarks. For SA-Co/VEval, we report CGF1 and pHOTA\nmetrics (defined in §G.5) across its subsets (SA-V, YT-Temporal-1B, SmartGlasses). For public\nbenchmarks, we use their official metrics. Baselines include GLEE, an open-vocabulary image and\nvideo segmentation model, “LLMDet + SAM 3 Tracker” (replacing our detector with LLMDet),\nand “SAM 3 Detector + T-by-D” (replacing our tracker with an association module based on the\ntracking-by-detection paradigm). In Tab. 5, SAM 3 largely outperforms these baselines, especially\non benchmarks with a very large number of noun phrases. On SA-Co/VEval it reaches over 80% of\nhuman pHOTA. See §G.5 for more details.\nSA-Co/VEval benchmark test split\npublic benchmarks\nSA-V\nYT-Temporal-1B\nSmartGlasses\nLVVIS\nBURST\nYTVIS21\nOVIS\n(2.0K NPs)\n(1.7K NPs)\n(2.4K NPs)\n(1.2K NPs)\n(482 NPs)\n(40 NPs)\n(25 NPs)\nModel\nCGF1\npHOTA\nCGF1\npHOTA\nCGF1\npHOTA\ntest mAP\ntest HOTA\nval mAP\nval mAP\nHuman\n53.2\n68.0\n73.8\n79.2\n57.6\n70.3\n–\n–\n–\n–\nGLEE† (all NPs at once)\n0.2\n8.8\n2.1\n18.9\n0.1\n5.5\n20.8\n28.4\n62.2\n38.7\nGLEE† (one NP at a time)\n0.0\n12.3\n3.2\n22.6\n0.1\n6.4\n9.3\n20.2\n56.5\n32.4\nLLMDet† + SAM 3 Tracker\n3.6\n31.2\n9.2\n41.5\n0.0\n7.4\n15.5\n35.1\n32.4\n47.5\nSAM 3 Detector + T-by-D\n22.2\n49.0\n44.6\n64.4\n29.3\n57.1\n37.3\n40.7\n57.3\n54.9\nSAM 3\n27.8\n53.9\n51.7\n69.2\n38.2\n62.9\n38.2\n45.9\n56.9\n59.9\nTable 5: Video PCS from a text prompt (open-vocabulary video instance segmentation) on SA-Co/VEval and\npublic benchmarks (see Tab. 38 for more results and analyses). SAM 3 shows strong performance, especially on\nbenchmarks with a large number of NPs. †: GLEE and LLMDet have not been trained on the SA-Co dataset, so\ntheir results should be seen as zero-shot on SA-Co/VEval.\nPVS. We evaluate SAM 3 on a range of geometric tasks, including Video Object Segmentation (VOS)\nand interactive image segmentation. Tab. 6 compares SAM 3 to recent state-of-the-art methods on the\nVOS task. SAM 3 achieves significant improvements over SAM 2 on most benchmarks, particularly\non the challenging MOSEv2 dataset, where SAM 3 outperforms prior work by 6 points. For the\ninteractive image segmentation task, we evaluate SAM 3 on the 37 datasets benchmark introduced in\nSAM 2. As shown in Tab. 7, SAM 3 outperforms SAM 2 on average mIoU.\nSAM 3 Agent.\nWe experiment with an MLLM that uses SAM 3 as a tool, called SAM 3 Agent,\nto segment more complex text queries such as “people sitting down but not holding a gift box in\n8\n",
      "text_length": 4760,
      "elements": [
        {
          "type": "figure",
          "bbox_1000": [
            571,
            98,
            824,
            251
          ],
          "bbox_pixels": [
            728,
            161,
            1050,
            414
          ],
          "label": "Figure 6",
          "description": "Line chart comparing SAM 3's interactive exemplar prompts vs. ideal PVS baseline on SA-Co, showing CGF1 score vs. number of box prompts, averaged over all SA-Co/Gold phrases. Four lines represent: SAM 3 PCS (blue), Perfect SAM 3 PVS (orange), Hybrid: 2 prompts PCS then PVS (teal), and Hybrid: 4 prompts PCS then PVS (magenta).",
          "crop_path": "elements/p08_figure_1_Figure_6.png"
        },
        {
          "type": "table",
          "bbox_1000": [
            565,
            344,
            833,
            438
          ],
          "bbox_pixels": [
            720,
            567,
            1062,
            722
          ],
          "label": "Table 4",
          "description": "Table showing accuracy on counting benchmarks (CountBench and PixMo-Count) for models including DINO-X, Qwen2-VL-72B, Molo-72B, Gemini 2.5 Pro, and SAM 3, with metrics MAE ↓ and Acc ↑. Gray shading indicates usage of training sets.",
          "crop_path": "elements/p08_table_2_Table_4.png"
        },
        {
          "type": "table",
          "bbox_1000": [
            175,
            603,
            824,
            735
          ],
          "bbox_pixels": [
            223,
            994,
            1050,
            1212
          ],
          "label": "Table 5",
          "description": "Table comparing video PCS performance (open-vocabulary video instance segmentation) on SA-Co/V Eval and public benchmarks for models including Human, GLEE, LLMdet + SAM 3 Tracker, SAM 3 Detector + T-by-D, and SAM 3. Metrics include CGF1, pHOTA, test mAP, val mAP, etc., across datasets like SA-V, YT-Temporal-1B, SmartGlasses, LVVIS, BURST, YTVIS21, and OVIS.",
          "crop_path": "elements/p08_table_3_Table_5.png"
        },
        {
          "type": "chart",
          "bbox_1000": [
            571,
            98,
            824,
            251
          ],
          "bbox_pixels": [
            728,
            161,
            1050,
            414
          ],
          "label": "Figure 6",
          "description": "Line chart comparing SAM 3's interactive exemplar prompts vs. ideal PVS baseline on SA-Co, showing CGF1 score vs. number of box prompts, averaged over all SA-Co/Gold phrases. Four lines represent: SAM 3 PCS (blue), Perfect SAM 3 PVS (orange), Hybrid: 2 prompts PCS then PVS (teal), and Hybrid: 4 prompts PCS then PVS (magenta).",
          "crop_path": "elements/p08_chart_4_Figure_6.png"
        }
      ]
    }
  ]
}