{
  "generated_date": "2025-12-15T22:40:17.592718",
  "pdfs": {
    "aiSeg_sam3_2025.pdf": {
      "pdf_path": "/home/shared/openlibrarymisc/EO_AI_fModel/contribs/aiSeg_sam3_2025.pdf",
      "total_pages": 68,
      "pages": {
        "1": {
          "image": "page_001.png",
          "traditional": {
            "text": "000\n001\n002\n003\n004\n005\n006\n007\n008\n009\n010\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n050\n051\n052\n053\nUnder review as a conference paper at ICLR 2026\nSAM 3: SEGMENT ANYTHING WITH CONCEPTS\nAnonymous authors\nPaper under double-blind review\nABSTRACT\nWe present Segment Anything Model (SAM) 3, a unified model that detects, seg-\nments, and tracks objects in images and videos based on concept prompts, which\nwe define as either short noun phrases (e.g., \u201cyellow school bus\u201d), image exemplars,\nor a combination of both. Promptable Concept Segmentation (PCS) takes such\nprompts and returns segmentation masks and unique identities for all matching\nobject instances. To advance PCS, we build a scalable data engine that produces\na high-quality dataset with 4M unique concept labels, including hard negatives,\nacross images and videos. Our model consists of a vision backbone shared be-\ntween an image-level detector and a memory-based video tracker. Recognition\nand localization are decoupled with a presence head, which significantly boosts\ndetection accuracy. SAM 3 delivers a 2\u00d7 gain over existing systems in both image\nand video PCS, and improves previous SAM capabilities in interactive visual seg-\nmentation tasks. We open source SAM 3 along with our new Segment Anything\nwith Concepts (SA-Co) benchmark.\n1\nINTRODUCTION\nThe ability to find and segment anything in a visual scene is foundational for multimodal AI, powering\napplications in robotics, content creation, augmented reality, data annotation, and scientific fields.\nThe SAM series (Kirillov et al., 2023; Ravi et al., 2024) introduced the promptable segmentation task\nto segment objects in images and videos via interactive prompts, including visual inputs like points,\nboxes, and masks marking a specific object, or text inputs describing an object. However, SAM 1\nand SAM 2 focus on visual prompts and segment a single object instance per prompt. While these\nmethods achieved a breakthrough for this critical task, they did not address the broader task of finding\nand segmenting all instances of a concept appearing anywhere in the input (e.g., all \u201ccats\u201d in a video).\nIn this work, we present SAM 3, a model that achieves a step change in promptable segmentation in\nimages and videos, improving Promptable Visual Segmentation (PVS) relative to SAM 2 and setting\na new standard for Promptable Concept Segmentation (PCS). We formalize the PCS task as taking\ntext and/or image exemplars as input, and predicting instance and semantic masks for every single\nobject matching the concept, while preserving object identities across video frames (\u00a72). We focus\non recognizing atomic visual concepts and thus constrain text to simple noun phrases (NPs), such as\n\u201cred apple\u201d or \u201cstriped cat\u201d. Example outputs are shown in Fig. 1.\nFigure 1: SAM 3 improves over SAM 2 on promptable visual segmentation with clicks (left) while advancing\npromptable concept segmentation (right) where users can segment all instances of a visual concept specified by\na short noun phrase, image exemplars, or a combination of both.\n1\n",
            "text_length": 3178,
            "image_count": 11,
            "block_count": 20,
            "link_count": 6
          },
          "multimodal": {
            "page_type": "content",
            "text_content": "Under review as a conference paper at ICLR 2026\n000\nSAM 3: SEGMENT ANYTHING WITH CONCEPTS\n001\n002\n003\n004\n005\n006\n007\n008\n009\n010\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n050\n051\n052\n053\nAnonymous authors\nPaper under double-blind review\nABSTRACT\nWe present Segment Anything Model (SAM) 3, a unified model that detects, seg-\nments, and tracks objects in images and videos based on concept prompts, which\nwe define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars,\nor a combination of both. Promptable Concept Segmentation (PCS) takes such\nprompts and returns segmentation masks and unique identities for all matching\nobject instances. To advance PCS, we build a scalable data engine that produces\na high-quality dataset with 4M unique concept labels, including hard negatives,\nacross images and videos. Our model consists of a vision backbone shared be-\ntween an image-level detector and a memory-based video tracker. Recognition\nand localization are decoupled with a presence head, which significantly boosts\ndetection accuracy. SAM 3 delivers a 2\u00d7 gain over existing systems in both image\nand video PCS, and improves previous SAM capabilities in interactive visual seg-\nmentation tasks. We open source SAM 3 along with our new Segment Anything\nwith Concepts (SA-Co) benchmark.\n1 INTRODUCTION\nThe ability to find and segment anything in a visual scene is foundational for multimodal AI, powering\napplications in robotics, content creation, augmented reality, data annotation, and scientific fields.\nThe SAM series (Kirillov et al., 2023; Ravi et al., 2024) introduced the promptable segmentation task\nto segment objects in images and videos via interactive prompts, including visual inputs like points,\nboxes, and masks marking a specific object, or text inputs describing an object. However, SAM 1\nand SAM 2 focus on visual prompts and segment a single object instance per prompt. While these\nmethods achieved a breakthrough for this critical task, they did not address the broader task of finding\nand segmenting all instances of a concept appearing anywhere in the input (e.g., all \"cats\" in a video).\nIn this work, we present SAM 3, a model that achieves a step change in promptable segmentation in\nimages and videos, improving Promptable Visual Segmentation (PVS) relative to SAM 2 and setting\na new standard for Promptable Concept Segmentation (PCS). We formalize the PCS task as taking\ntext and/or image exemplars as input, and predicting instance and semantic masks for every single\nobject matching the concept, while preserving object identities across video frames (\u00a72). We focus\non recognizing atomic visual concepts and thus constrain text to simple noun phrases (NPs), such as\n\"red apple\" or \"striped cat\". Example outputs are shown in Fig. 1.\nPROMPTABLE VISUAL SEGMENTATION\nPROMPTABLE CONCEPT SEGMENTATION\na striped cat\na round cell\nsmall window\nVIDEO\nIMAGE\na kangaroo\nhard hat\nPrompts: positive or negative points\nPrompts: noun phrase and/or positive or negative image exemplar\nFigure 1: SAM 3 improves over SAM 2 on promptable visual segmentation with clicks (left) while advancing\npromptable concept segmentation (right) where users can segment all instances of a visual concept specified by\na short noun phrase, image exemplars, or a combination of both.\n1",
            "figures": [
              {
                "number": "1",
                "caption": "Figure 1: SAM 3 improves over SAM 2 on promptable visual segmentation with clicks (left) while advancing\npromptable concept segmentation (right) where users can segment all instances of a visual concept specified by\na short noun phrase, image exemplars, or a combination of both.",
                "description": "The figure demonstrates the improvement of SAM 3 over SAM 2. On the left, the 'promptable visual segmentation' shows an object (unspecified but visually appears to be sea slug-like creature) segmented using positive or negative prompts, shown as green and red points. The video examples below also depict the use of positive and negative points for segmentation.\nOn the right, the 'promptable concept segmentation' illustrates the ability to specify a concept and segment all instances of the concept in images and videos. Examples are provided for:\n- A striped cat (image): The segmentation highlights multiple cats in an image.\n- A round cell (image): The segmentation highlights multiple cells.\n- A small window (image): The segmentation highlights the window.\n- A kangaroo (video): The segmentation highlights kangaroo.\n- A hard hat (video): The segmentation highlights the hard hat.\nThe prompts used for segmentation are listed below each example, including noun phrases, positive/negative prompts, and image exemplars."
              }
            ],
            "tables": [],
            "equations": [],
            "key_concepts": [
              "Segment Anything Model (SAM)",
              "Promptable Concept Segmentation (PCS)",
              "Promptable Visual Segmentation (PVS)",
              "interactive prompts",
              "concept prompts",
              "image exemplars",
              "segmentation masks"
            ],
            "citations_mentioned": [
              "Kirillov et al., 2023",
              "Ravi et al., 2024"
            ]
          }
        }
      }
    }
  }
}